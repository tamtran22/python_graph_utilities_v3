{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = '/data1/tam/datasets/10081/CFD_1D/Output_10081_Amount_St_whole.dat'\n",
    "# data_input = read_1D_input(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = '/data1/tam/datasets'\n",
    "# subject = '10081'\n",
    "# times = [str(i).zfill(3) for i in range(201)]\n",
    "# file_name_output = lambda subject, time : f'{raw}/{subject}'+\\\n",
    "#             f'/CFD_1D/data_plt_nd/plt_nd_000{time}.dat'\n",
    "# file_names = [file_name_output(subject, time) for time in times]\n",
    "\n",
    "# data_output = read_1D_output(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {}\n",
    "# for key in data_input:\n",
    "#     print(key, data_input[key].shape)\n",
    "#     data[key] = data_input[key]\n",
    "# for key in data_output:\n",
    "#     print(key, data_output[key].shape)\n",
    "#     data[key] = data_output[key]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = process_data(data=data, max_length=5.)\n",
    "# for key in out:\n",
    "#     print(key, out[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.graph_dataset import OneDDatasetBuilder\n",
    "\n",
    "# dataset = OneDDatasetBuilder(\n",
    "#     raw_dir='/data1/tam/datasets',\n",
    "#     root_dir='/data1/tam/downloaded_datasets_v2',\n",
    "#     sub_dir='processed',\n",
    "#     subjects='all',\n",
    "#     refined_max_length=4.,\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)],\n",
    "#     data_type = torch.float64,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.graph_dataset import OneDDatasetLoader\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_v2',\n",
    "    sub_dir='processed',\n",
    "    subjects=['10081'],\n",
    "    time_names=[str(i).zfill(3) for i in range(201)],\n",
    "    data_type = torch.float64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.pre_process import normalize\n",
    "# normalized_dataset = normalize(dataset,\n",
    "#     sub_dir='normalized',\n",
    "#     scaler_dict={\n",
    "#         'node_attr' : ['minmax_scaler']*9,\n",
    "#         'edge_attr' : ['minmax_scaler']*15,\n",
    "#         'pressure' : 'minmax_scaler',\n",
    "#     },\n",
    "#     clipping=5e-4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.graph_dataset import OneDDatasetLoader\n",
    "# dataset = OneDDatasetLoader(\n",
    "#     root_dir='/data1/tam/downloaded_datasets_v2',\n",
    "#     sub_dir='normalized',\n",
    "#     subjects=['10081'],\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)],\n",
    "#     data_type = torch.float64,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.pre_process import batchgraph_generation_wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dataset = batchgraph_generation_wise(\n",
    "    dataset=dataset,\n",
    "    batch_gens=[[0,5]],\n",
    "    subset_hops=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 TorchGraphData(edge_index=[2, 164], edge_index_raw=[2, 59814], original_flag=[165], node_attr=[165, 9], edge_attr=[164, 15], pressure=[165, 201], flowrate=[165, 201])\n"
     ]
    }
   ],
   "source": [
    "for i in range(batched_dataset.len()):\n",
    "    print(i, batched_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchGraphData(edge_index=[2, 164], edge_index_raw=[2, 59814], original_flag=[165], node_attr=[165, 9], edge_attr=[164, 15], pressure=[165, 201], flowrate=[165, 201])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = batched_dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3., 4.]), array([34, 21, 23, 26, 60]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.edge_attr[:,2].numpy()\n",
    "import numpy as np\n",
    "np.unique(y, return_counts=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tam_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
