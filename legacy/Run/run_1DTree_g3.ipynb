{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../Codes_1Dtree')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from data.graph_dataset import OneDDatasetBuilder, OneDDatasetLoader, normalize\n",
    "# from networks.gcn import GraphUNet, RecurrentFormulationNet\n",
    "from networks.gcnv4 import RecurrentFormulationNet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "from data.graph_dataset import batchgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d) -> None:\n",
    "        self.__dict__ = d\n",
    "    def setattr(self, attr_name, attr_value):\n",
    "        self.__dict__[attr_name] = attr_value\n",
    "\n",
    "args = objectview({\n",
    "    'n_field': 1,\n",
    "    'n_meshfield': 16,\n",
    "    'hidden_size': 256,\n",
    "    'latent_size': 256,\n",
    "    'aggr': 'sum',\n",
    "    'act': 'mish',\n",
    "    'dropout': 0.2,\n",
    "    'device': torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "    'lr': 1e-6,\n",
    "    'weight_decay': 1e-3,\n",
    "    'n_epoch': 500,\n",
    "    'alpha': 0.5,\n",
    "    'batchsize': 20000,\n",
    "    'timestep': 201,\n",
    "    'timeslice_hops': 0,\n",
    "    'timeslice_steps': 5,\n",
    "    'n_data_per_batch': 1,\n",
    "    'criterion': torch.nn.MSELoss(),\n",
    "    'plot': False\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = OneDDatasetBuilder(\n",
    "#     raw_dir='/data1/tam/datasets',\n",
    "#     root_dir='/data1/tam/downloaded_datasets_Static_v1',\n",
    "#     sub_dir='processed',\n",
    "#     subjects='all',\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)],\n",
    "#     data_type = torch.float32,\n",
    "#     readme='edge_index(2xn_edge), node_attr(n_nodex10), pressure+flowrate(n_nodex201)'\n",
    "# )\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_Static_v1',\n",
    "    sub_dir='processed',\n",
    "    subjects='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)],\n",
    "    data_type = torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = normalize(\n",
    "    dataset=dataset,\n",
    "    sub_dir='normalized',\n",
    "    scaler_dict={\n",
    "        'node_attr': ('minmax_scaler', 0, None),\n",
    "        'pressure': ('robust_scaler', None, None),\n",
    "        # 'flowrate': ('robust_scaler', None, None),\n",
    "        'pressure_dot': ('robust_scaler', None, None),\n",
    "        # 'flowrate_dot': ('robust_scaler', None, None),\n",
    "        # 'time': ('minmax_scaler', None, None)\n",
    "    }\n",
    ")\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_Static_v1',\n",
    "    sub_dir='normalized',\n",
    "    subjects='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)],\n",
    "    data_type = torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = batchgraph_generation_wise(\n",
    "#     dataset,\n",
    "#     sub_dir='batched',\n",
    "#     batch_gens=[[14,15]],\n",
    "#     timestep=args.timestep,\n",
    "#     timeslice_hops=args.timeslice_hops,\n",
    "#     timeslice_steps=args.timeslice_steps\n",
    "# )\n",
    "dataset = batchgraph(\n",
    "    dataset,\n",
    "    sub_dir='batched_1',\n",
    "    batchsize=None,\n",
    "    timestep=args.timestep,\n",
    "    timeslice_hops=args.timeslice_hops,\n",
    "    timeslice_steps=args.timeslice_steps\n",
    ")\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_Static_v1',\n",
    "    sub_dir='batched_1',\n",
    "    subjects='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)],\n",
    "    data_type = torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_size = 2\n",
    "fold = [list(range(i*fold_size, (i+1)*fold_size)) for i in range(8)]\n",
    "n_train_fold = 5\n",
    "\n",
    "# for i in range(n_train_fold):\n",
    "#     test_subset = fold[i]\n",
    "#     train_subset = []\n",
    "#     for j in range(n_train_fold):\n",
    "#         if j != i:\n",
    "#             train_subset += fold[j]\n",
    "\n",
    "# (train_loader, test_loader) = dataset_to_loader(\n",
    "#     dataset=dataset,\n",
    "#     data_subset_dict={\n",
    "#         'train': list(range(5, 6)),\n",
    "#         'test': list(range(0, 1))\n",
    "#     },\n",
    "#     n_data_per_batch=args.n_data_per_batch\n",
    "# )\n",
    "\n",
    "# train_set, test_set = dataset_to_loader(\n",
    "#     dataset=dataset,\n",
    "#     data_subset_dict={\n",
    "#         'train': list(range(6, 36)),\n",
    "#         'test': list(range(0, 5))\n",
    "#     },\n",
    "#     n_data_per_batch=args.n_data_per_batch\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initializing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentFormulationNet(\n",
    "    n_field=args.n_field,\n",
    "    n_meshfield=args.n_meshfield,\n",
    "    hidden_size=args.hidden_size,\n",
    "    latent_size=args.latent_size,\n",
    "    act=args.act,\n",
    "    use_time_feature=True,\n",
    "    dropout=args.dropout,\n",
    "    use_hidden=True\n",
    ")\n",
    "setattr(model, 'name', 'PARC_GCN_UNet_full')\n",
    "model = model.to(args.device)\n",
    "model.load_state_dict(torch.load(f'models/{model.name}_node2_epoch500.pth', map_location=args.device) )\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "setattr(args, 'optimizer', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, args):\n",
    "    ##\n",
    "    F_true = torch.cat([data.pressure.unsqueeze(2)], dim=2) \\\n",
    "                .float().to(args.device)\n",
    "    F_dot_true = torch.cat([data.pressure_dot.unsqueeze(2)], dim=2) \\\n",
    "                .float().to(args.device)\n",
    "    ##\n",
    "    F_0 = F_true[:,0,:]\n",
    "    # edge_index = data.edge_index.to(args.device)\n",
    "    edge_index = torch.cat([data.edge_index, torch.flip(data.edge_index, dims=[0])], dim=1).to(args.device)\n",
    "    node_attr = data.node_attr.float().to(args.device)\n",
    "    F_true = F_true[:,1:,:]\n",
    "    F_dot_true = F_dot_true[:,1:,:]\n",
    "    time = data.time.float().to(args.device)\n",
    "    ##\n",
    "    F_pred, F_dot_pred = model.forward(\n",
    "        F_0=F_0,\n",
    "        edge_index=edge_index,\n",
    "        meshfield=node_attr,\n",
    "        time=time,\n",
    "        n_time=data.number_of_timesteps - 1,\n",
    "        device=args.device\n",
    "    )\n",
    "    ##\n",
    "    loss = args.criterion(F_pred, F_true)*args.alpha + (1.-args.alpha)*args.criterion(F_dot_pred, F_dot_true)\n",
    "    loss.backward()\n",
    "    args.optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def eval(model, data, args):\n",
    "    ##\n",
    "    F_true = torch.cat([data.pressure.unsqueeze(2)], dim=2) \\\n",
    "                .float().to(args.device)\n",
    "    F_dot_true = torch.cat([data.pressure_dot.unsqueeze(2)], dim=2) \\\n",
    "                .float().to(args.device)\n",
    "    ##\n",
    "    F_0 = F_true[:,0,:]\n",
    "    # edge_index = data.edge_index.to(args.device)\n",
    "    edge_index = torch.cat([data.edge_index, torch.flip(data.edge_index, dims=[0])], dim=1).to(args.device)\n",
    "    node_attr = data.node_attr.float().to(args.device)\n",
    "    F_true = F_true[:,1:,:]\n",
    "    F_dot_true = F_dot_true[:,1:,:]\n",
    "    time = data.time.float().to(args.device)\n",
    "    ##\n",
    "    with torch.no_grad():\n",
    "        F_pred, F_dot_pred = model.forward(\n",
    "            F_0=F_0,\n",
    "            edge_index=edge_index,\n",
    "            meshfield=node_attr,\n",
    "            time=time,\n",
    "            n_time=data.number_of_timesteps - 1,\n",
    "            device=args.device\n",
    "        )\n",
    "        loss = args.criterion(F_pred, F_true)*args.alpha + (1.-args.alpha)*args.criterion(F_dot_pred, F_dot_true)\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss = 3.4960363090038293; eval loss = 4.376752078533173\n",
      "Epoch 9: train loss = 3.456063965459664; eval loss = 4.333602366348107\n",
      "Epoch 14: train loss = 3.491986308991909; eval loss = 4.329758994281292\n",
      "Epoch 19: train loss = 3.4658332392573357; eval loss = 4.333074827988942\n",
      "Epoch 24: train loss = 3.469724143544833; eval loss = 4.352372484902542\n",
      "Epoch 29: train loss = 3.490648339192073; eval loss = 4.356131213406723\n",
      "Epoch 34: train loss = 3.5297192454338067; eval loss = 4.380157118042311\n",
      "Epoch 39: train loss = 3.504964096844199; eval loss = 4.388063937425613\n",
      "Epoch 44: train loss = 3.510457652310528; eval loss = 4.383433627585571\n",
      "Epoch 49: train loss = 3.5420380170146606; eval loss = 4.440773792564869\n",
      "Epoch 54: train loss = 3.5342303360501943; eval loss = 4.412063437203566\n",
      "Epoch 59: train loss = 3.647164194782576; eval loss = 4.568718279401461\n",
      "Epoch 64: train loss = 3.921804159382976; eval loss = 4.953121195236843\n",
      "Epoch 69: train loss = 4.521487582226594; eval loss = 5.695551897088686\n",
      "Epoch 74: train loss = 4.839120108882585; eval loss = 5.985833371678988\n",
      "Epoch 79: train loss = 3.814243525763351; eval loss = 4.679188956816991\n",
      "Epoch 84: train loss = 3.8532190943757705; eval loss = 4.809723707536856\n",
      "Epoch 89: train loss = 4.06553371200959; eval loss = 5.082546710968016\n",
      "Epoch 94: train loss = 3.9989440560340888; eval loss = 4.990068624416987\n",
      "Epoch 99: train loss = 4.171615563333035; eval loss = 5.23879769941171\n",
      "Epoch 104: train loss = 4.081177130838235; eval loss = 5.104131626586119\n",
      "Epoch 109: train loss = 3.8022915204366052; eval loss = 4.740748628973961\n",
      "Epoch 114: train loss = 3.9909178197383888; eval loss = 4.973312815030415\n",
      "Epoch 119: train loss = 3.7862523371974626; eval loss = 4.714963927865029\n",
      "Epoch 124: train loss = 4.5479196101427055; eval loss = 5.81343245009581\n",
      "Epoch 129: train loss = 3.751309869686761; eval loss = 4.673370371262234\n",
      "Epoch 134: train loss = 3.7212728569904963; eval loss = 4.6723551750183105\n",
      "Epoch 139: train loss = 3.862142777442932; eval loss = 4.822670842210452\n",
      "Epoch 144: train loss = 3.7165272027254117; eval loss = 4.630720327297846\n",
      "Epoch 149: train loss = 3.7742960179845513; eval loss = 4.764552935957907\n",
      "Epoch 154: train loss = 4.145155644416809; eval loss = 5.117088253299394\n",
      "Epoch 159: train loss = 3.8388795475165045; eval loss = 4.803256769975025\n",
      "Epoch 164: train loss = 3.6806091626485182; eval loss = 4.639368072152138\n",
      "Epoch 169: train loss = 3.917793829242389; eval loss = 4.929163118203481\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "total_train_loss = []\n",
    "total_eval_loss = []\n",
    "train_loss = 0\n",
    "eval_loss = 0\n",
    "for epoch in range(args.n_epoch):\n",
    "    CUDA_LAUNCH_BLOCKING=1\n",
    "    torch.cuda.empty_cache()\n",
    "    ##\n",
    "    test_subset = fold[epoch % n_train_fold]\n",
    "    train_subset = []\n",
    "    for j in range(n_train_fold):\n",
    "        if j != (epoch % n_train_fold):\n",
    "            train_subset += fold[j]\n",
    "    ##\n",
    "    torch.cuda.empty_cache()\n",
    "    # train_loss = 0\n",
    "    for i_data in train_subset:\n",
    "        data = dataset[i_data]\n",
    "        train_loader = NeighborLoader(data, num_neighbors=[1], batch_size=args.batchsize)\n",
    "        for i in range(train_loader.__len__()):\n",
    "            data = next(iter(train_loader))\n",
    "            train_loss += train(model=model, data=data, args=args) / train_loader.__len__()\n",
    "\n",
    "    # eval_loss = 0\n",
    "    for i_data in test_subset:\n",
    "        data = dataset[i_data]\n",
    "        test_loader = NeighborLoader(data, num_neighbors=[1], batch_size=args.batchsize)\n",
    "        for i in range(test_loader.__len__()):\n",
    "            data = next(iter(test_loader))\n",
    "            eval_loss += eval(model=model, data=data, args=args) / test_loader.__len__()\n",
    "    \n",
    "    if (epoch+1) % n_train_fold == 0:\n",
    "        train_loss /= n_train_fold*fold_size\n",
    "        eval_loss /= fold_size\n",
    "        print(f'Epoch {epoch}: train loss = {train_loss}; eval loss = {eval_loss}')\n",
    "        train_loss = 0\n",
    "        eval_loss = 0\n",
    "    # else:\n",
    "        # print(f'Epoch {epoch}.')\n",
    "        \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        torch.save(model.state_dict(), f'models/{model.name}_node2_epoch{epoch+1}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tam_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
