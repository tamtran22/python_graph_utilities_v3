{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../Codes_1Dtree')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from data.graph_dataset import OneDDatasetBuilder, OneDDatasetLoader, normalize, dataset_to_loader\n",
    "# from networks.gcn import GraphUNet, RecurrentFormulationNet\n",
    "from networks.gcnv2 import GraphUNetv2, RecurrentFormulationNet_hidden\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.graph_dataset import batchgraph_generation_wise, batchgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d) -> None:\n",
    "        self.__dict__ = d\n",
    "    def setattr(self, attr_name, attr_value):\n",
    "        self.__dict__[attr_name] = attr_value\n",
    "\n",
    "args = objectview({\n",
    "    'n_field': 2,\n",
    "    'n_meshfield': 16,\n",
    "    'hidden_size': 128,\n",
    "    'latent_size': 128,\n",
    "    'aggr': 'sum',\n",
    "    'act': 'mish',\n",
    "    'dropout': 0.2,\n",
    "    'device': torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "    'lr': 1e-6,\n",
    "    'weight_decay': 5e-3,\n",
    "    'n_epoch': 200,\n",
    "    'alpha': 0.5,\n",
    "    'timestep': None,\n",
    "    'timeslice_hops': 1,\n",
    "    'timeslice_steps': 1,\n",
    "    'n_data_per_batch': 3,\n",
    "    'criterion': torch.nn.MSELoss(),\n",
    "    'plot': False\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = OneDDatasetBuilder(\n",
    "#     raw_dir='/data1/tam/datasets',\n",
    "#     root_dir='/data1/tam/downloaded_datasets_Static_v1',\n",
    "#     sub_dir='processed',\n",
    "#     subjects='all',\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)],\n",
    "#     data_type = torch.float32,\n",
    "#     readme='edge_index(2xn_edge), node_attr(n_nodex10), pressure+flowrate(n_nodex201)'\n",
    "# )\n",
    "# dataset = OneDDatasetLoader(\n",
    "#     root_dir='/data1/tam/downloaded_datasets_Static_v1',\n",
    "#     sub_dir='processed',\n",
    "#     subjects='all',\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)],\n",
    "#     data_type = torch.float32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = normalize(\n",
    "#     dataset=dataset,\n",
    "#     sub_dir='normalized',\n",
    "#     scaler_dict={\n",
    "#         'node_attr': ('minmax_scaler', 0, None),\n",
    "#         'pressure': ('minmax_scaler', None, 0.99),\n",
    "#         'flowrate': ('minmax_scaler', None, 0.99),\n",
    "#         'pressure_dot': ('minmax_scaler', None, 0.99),\n",
    "#         'flowrate_dot': ('minmax_scaler', None, 0.99),\n",
    "#         'time': ('minmax_scaler', None, None)\n",
    "#     }\n",
    "# )\n",
    "# dataset = OneDDatasetLoader(\n",
    "#     root_dir='/data1/tam/downloaded_datasets_Static_v1',\n",
    "#     sub_dir='normalized',\n",
    "#     subjects='all',\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)],\n",
    "#     data_type = torch.float32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = batchgraph_generation_wise(\n",
    "#     dataset,\n",
    "#     sub_dir='batched',\n",
    "#     batch_gens=[[14,15]],\n",
    "#     timestep=args.timestep,\n",
    "#     timeslice_hops=args.timeslice_hops,\n",
    "#     timeslice_steps=args.timeslice_steps\n",
    "# )\n",
    "# dataset = batchgraph(\n",
    "#     dataset,\n",
    "#     sub_dir='batched',\n",
    "#     batchsize=1000,\n",
    "#     timestep=args.timestep,\n",
    "#     timeslice_hops=args.timeslice_hops,\n",
    "#     timeslice_steps=args.timeslice_steps\n",
    "# )\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_Static_v1',\n",
    "    sub_dir='batched',\n",
    "    subjects='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)],\n",
    "    data_type = torch.float32\n",
    ")\n",
    "if args.plot:\n",
    "    print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [dataset[i] for i in range(dataset.len())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_size = 5\n",
    "# fold = [list(range(i*fold_size, (i+1)*fold_size) for i in range(5))]\n",
    "# (train_loader, test_loader) = dataset_to_loader(\n",
    "#     dataset=dataset,\n",
    "#     data_subset_dict={\n",
    "#         'train': list(range(5, 35)),\n",
    "#         'test': list(range(0, 5))\n",
    "#     },\n",
    "#     n_data_per_batch=args.n_data_per_batch\n",
    "# )\n",
    "\n",
    "train_set, test_set = dataset_to_loader(\n",
    "    dataset=dataset,\n",
    "    data_subset_dict={\n",
    "        'train': list(range(6, 36)),\n",
    "        'test': list(range(0, 5))\n",
    "    },\n",
    "    n_data_per_batch=args.n_data_per_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initializing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentFormulationNet_hidden(\n",
    "    n_field=args.n_field,\n",
    "    n_meshfield=args.n_meshfield,\n",
    "    hidden_size=args.hidden_size,\n",
    "    latent_size=args.latent_size,\n",
    "    act=args.act,\n",
    "    use_time_feature=True,\n",
    "    dropout=args.dropout\n",
    ")\n",
    "setattr(model, 'name', 'PARC_GCN_UNet_full')\n",
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "setattr(args, 'optimizer', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, args):\n",
    "    ##\n",
    "    F_true = torch.cat([data.pressure.unsqueeze(2), data.flowrate.unsqueeze(2)], dim=2) \\\n",
    "                .float().to(args.device)\n",
    "    F_dot_true = torch.cat([data.pressure_dot.unsqueeze(2), data.flowrate_dot.unsqueeze(2)], dim=2) \\\n",
    "                .float().to(args.device)\n",
    "    ##\n",
    "    F_0 = F_true[:,0,:]\n",
    "    # edge_index = data.edge_index.to(args.device)\n",
    "    edge_index = torch.cat([data.edge_index, torch.flip(data.edge_index, dims=[0])], dim=1).to(args.device)\n",
    "    node_attr = data.node_attr.float().to(args.device)\n",
    "    F_true = F_true[:,1:,:]\n",
    "    F_dot_true = F_dot_true[:,1:,:]\n",
    "    time = data.time.float().to(args.device)\n",
    "    ##\n",
    "    F_pred, F_dot_pred = model.forward(\n",
    "        F_0=F_0,\n",
    "        edge_index=edge_index,\n",
    "        meshfield=node_attr,\n",
    "        time=time,\n",
    "        n_time=data.number_of_timesteps - 1,\n",
    "        device=args.device\n",
    "    )\n",
    "    ##\n",
    "    loss = args.criterion(F_pred, F_true)*args.alpha + (1.-args.alpha)*args.criterion(F_dot_pred, F_dot_true)\n",
    "    loss.backward()\n",
    "    args.optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def eval(model, data, args):\n",
    "    ##\n",
    "    F_true = torch.cat([data.pressure.unsqueeze(2), data.flowrate.unsqueeze(2)], dim=2) \\\n",
    "                .float().to(args.device)\n",
    "    F_dot_true = torch.cat([data.pressure_dot.unsqueeze(2), data.flowrate_dot.unsqueeze(2)], dim=2) \\\n",
    "                .float().to(args.device)\n",
    "    ##\n",
    "    F_0 = F_true[:,0,:]\n",
    "    # edge_index = data.edge_index.to(args.device)\n",
    "    edge_index = torch.cat([data.edge_index, torch.flip(data.edge_index, dims=[0])], dim=1).to(args.device)\n",
    "    node_attr = data.node_attr.float().to(args.device)\n",
    "    F_true = F_true[:,1:,:]\n",
    "    F_dot_true = F_dot_true[:,1:,:]\n",
    "    time = data.time.float().to(args.device)\n",
    "    ##\n",
    "    with torch.no_grad():\n",
    "        F_pred, F_dot_pred = model.forward(\n",
    "            F_0=F_0,\n",
    "            edge_index=edge_index,\n",
    "            meshfield=node_attr,\n",
    "            time=time,\n",
    "            n_time=data.number_of_timesteps - 1,\n",
    "            device=args.device\n",
    "        )\n",
    "        loss = args.criterion(F_pred, F_true)*args.alpha + (1.-args.alpha)*args.criterion(F_dot_pred, F_dot_true)\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlfm/anaconda3/envs/geometric/lib/python3.11/site-packages/torch_geometric/utils/sparse.py:176: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)\n",
      "  return adj.to_sparse_csr()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.07876851679457827; eval loss = 0.23630165060361227\n",
      "Epoch 1: train loss = 0.06922524499308236; eval loss = 0.22864992991842406\n",
      "Epoch 2: train loss = 0.06325348011531604; eval loss = 0.2242613567246331\n",
      "Epoch 3: train loss = 0.05898486746572803; eval loss = 0.22050368364411171\n",
      "Epoch 4: train loss = 0.05545914284065492; eval loss = 0.21554821789866746\n",
      "Epoch 5: train loss = 0.05085510195763986; eval loss = 0.21069835623105368\n",
      "Epoch 6: train loss = 0.04728097640066179; eval loss = 0.2077354072320341\n",
      "Epoch 7: train loss = 0.044964717666408936; eval loss = 0.20587033906368293\n",
      "Epoch 8: train loss = 0.04332936475094402; eval loss = 0.20446842141223676\n",
      "Epoch 9: train loss = 0.042107915540435394; eval loss = 0.20329479797921998\n",
      "Epoch 10: train loss = 0.041008505547914006; eval loss = 0.20239336156483853\n",
      "Epoch 11: train loss = 0.0400857713432784; eval loss = 0.20141118508998793\n",
      "Epoch 12: train loss = 0.039432009868008634; eval loss = 0.19992632351138376\n",
      "Epoch 13: train loss = 0.03901096548752736; eval loss = 0.19830365102700512\n",
      "Epoch 14: train loss = 0.03907293986447168; eval loss = 0.1952304197381241\n",
      "Epoch 15: train loss = 0.03961733400241776; eval loss = 0.19519171419769826\n",
      "Epoch 16: train loss = 0.039289475300439115; eval loss = 0.19437803358140618\n",
      "Epoch 17: train loss = 0.03809171892992534; eval loss = 0.19347314551623182\n",
      "Epoch 18: train loss = 0.036994438044815094; eval loss = 0.19192841603900446\n",
      "Epoch 19: train loss = 0.0363956629511366; eval loss = 0.19027971830030885\n",
      "Epoch 20: train loss = 0.03637733017354447; eval loss = 0.18945154833673228\n",
      "Epoch 21: train loss = 0.0368566717920973; eval loss = 0.18965223582104\n",
      "Epoch 22: train loss = 0.03742651655866611; eval loss = 0.18930536418250113\n",
      "Epoch 23: train loss = 0.03805207746030917; eval loss = 0.18932039840052825\n",
      "Epoch 24: train loss = 0.03890878393893113; eval loss = 0.1898370123270786\n",
      "Epoch 25: train loss = 0.03985232660663511; eval loss = 0.19130195617073714\n",
      "Epoch 26: train loss = 0.040945495401592626; eval loss = 0.19165828733733206\n",
      "Epoch 27: train loss = 0.04120653017397258; eval loss = 0.1929429370646525\n",
      "Epoch 28: train loss = 0.04109900314896643; eval loss = 0.19229629816430988\n",
      "Epoch 29: train loss = 0.04074030394733658; eval loss = 0.1914389550384849\n",
      "Epoch 30: train loss = 0.040132525589714196; eval loss = 0.18966100116570792\n",
      "Epoch 31: train loss = 0.03881922506060697; eval loss = 0.18869785061388306\n",
      "Epoch 32: train loss = 0.0373893503837174; eval loss = 0.19144676655831963\n",
      "Epoch 33: train loss = 0.036896121922987565; eval loss = 0.19248419638836023\n",
      "Epoch 34: train loss = 0.03609685413224241; eval loss = 0.19098314010735715\n",
      "Epoch 35: train loss = 0.03505950629106028; eval loss = 0.18853524703570088\n",
      "Epoch 36: train loss = 0.03412085852243775; eval loss = 0.1864870654212104\n",
      "Epoch 37: train loss = 0.033031493318212014; eval loss = 0.18449911369819832\n",
      "Epoch 38: train loss = 0.03220575773700845; eval loss = 0.18292134955073847\n",
      "Epoch 39: train loss = 0.032028390160823234; eval loss = 0.18175629428540818\n",
      "Epoch 40: train loss = 0.03228110814381977; eval loss = 0.18268573374459238\n",
      "Epoch 41: train loss = 0.032602316937874215; eval loss = 0.1839822365178002\n",
      "Epoch 42: train loss = 0.03256640263065671; eval loss = 0.18448251801909823\n",
      "Epoch 43: train loss = 0.03222469834204253; eval loss = 0.18413427923664902\n",
      "Epoch 44: train loss = 0.03203449952345209; eval loss = 0.18357409356218396\n",
      "Epoch 45: train loss = 0.032791832881127116; eval loss = 0.18543395610770794\n",
      "Epoch 46: train loss = 0.04082561255308174; eval loss = 0.1848618606425295\n",
      "Epoch 47: train loss = 0.034213335740233435; eval loss = 0.18638775625614204\n",
      "Epoch 48: train loss = 0.03419353956649751; eval loss = 0.18823086026341024\n",
      "Epoch 49: train loss = 0.033859282958931126; eval loss = 0.18801361849211684\n",
      "Epoch 50: train loss = 0.034230332103023675; eval loss = 0.18825409945213434\n",
      "Epoch 51: train loss = 0.033328843020949066; eval loss = 0.1886135238890696\n",
      "Epoch 52: train loss = 0.032921623986989716; eval loss = 0.1872471437008694\n",
      "Epoch 53: train loss = 0.033226047593677914; eval loss = 0.1865220968470429\n",
      "Epoch 54: train loss = 0.03307362977421627; eval loss = 0.1858187775419216\n",
      "Epoch 55: train loss = 0.03264664313968668; eval loss = 0.18520513297331453\n",
      "Epoch 56: train loss = 0.03240948068308951; eval loss = 0.18329115482893857\n",
      "Epoch 57: train loss = 0.031944730037007034; eval loss = 0.18267408088602202\n",
      "Epoch 58: train loss = 0.031415213583361884; eval loss = 0.18178902370761138\n",
      "Epoch 59: train loss = 0.03100611844341162; eval loss = 0.18178816245059776\n",
      "Epoch 60: train loss = 0.030416957277634424; eval loss = 0.182598251284975\n",
      "Epoch 61: train loss = 0.029857681342440007; eval loss = 0.18289492542695518\n",
      "Epoch 62: train loss = 0.029276910642959342; eval loss = 0.18197676900661353\n",
      "Epoch 63: train loss = 0.02878303250357786; eval loss = 0.18198385804590553\n",
      "Epoch 64: train loss = 0.029129671894152557; eval loss = 0.18276660023915647\n",
      "Epoch 65: train loss = 0.029882213557785135; eval loss = 0.18371212663072528\n",
      "Epoch 66: train loss = 0.02999351332484163; eval loss = 0.1841062493998595\n",
      "Epoch 67: train loss = 0.029481657870529872; eval loss = 0.18413855511732777\n",
      "Epoch 68: train loss = 0.02874172218585176; eval loss = 0.18405662281344634\n",
      "Epoch 69: train loss = 0.02811343408805868; eval loss = 0.18417336603607795\n",
      "Epoch 70: train loss = 0.027787084327206998; eval loss = 0.18462028497397298\n",
      "Epoch 71: train loss = 0.027705951860133565; eval loss = 0.18565331294079018\n",
      "Epoch 72: train loss = 0.027866160859513; eval loss = 0.18666688616227622\n",
      "Epoch 73: train loss = 0.028119837546913353; eval loss = 0.18753263203784673\n",
      "Epoch 74: train loss = 0.028288959356910486; eval loss = 0.1883751576898074\n",
      "Epoch 75: train loss = 0.028500835390764645; eval loss = 0.18928560659740912\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "total_train_loss = []\n",
    "total_eval_loss = []\n",
    "for epoch in range(args.n_epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loss = 0\n",
    "    for i in range(train_loader.__len__()):\n",
    "        data = next(iter(train_loader))\n",
    "        train_loss += train(model=model, data=data, args=args)\n",
    "    train_loss /= train_loader.__len__()\n",
    "    total_train_loss.append(train_loss)\n",
    "\n",
    "    eval_loss = 0\n",
    "    for i in range(test_loader.__len__()):\n",
    "        data = next(iter(test_loader))\n",
    "        eval_loss += eval(model=model, data=data, args=args)\n",
    "    eval_loss /= test_loader.__len__()\n",
    "    total_eval_loss.append(eval_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch}: train loss = {train_loss}; eval loss = {eval_loss}')\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        torch.save(model.state_dict(), f'models/{model.name}_node0_epoch{epoch+1}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
