{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../Codes_1Dtree')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from data.graph_dataset import OneDDatasetBuilder, OneDDatasetLoader, normalize\n",
    "# from networks.gcn import GraphUNet, RecurrentFormulationNet\n",
    "from networks.gcnv5 import RecurrentFormulationNet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from networks.losses import LpLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d) -> None:\n",
    "        self.__dict__ = d\n",
    "    def setattr(self, attr_name, attr_value):\n",
    "        self.__dict__[attr_name] = attr_value\n",
    "\n",
    "args = objectview({\n",
    "    'n_field': 1,\n",
    "    'n_meshfield': (3, 13),\n",
    "    'hidden_size': 256,\n",
    "    'n_hidden': 10,\n",
    "    'n_time': 2,\n",
    "    'aggr': 'sum',\n",
    "    'act': 'leakyrelu',\n",
    "    'dropout': 0.1,\n",
    "    'use_hidden': True,\n",
    "    'device': torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "    'lr': 1e-4,\n",
    "    # 'lr_decay': 0.5,\n",
    "    # 'lr_decay_period': 50,\n",
    "    'weight_decay': 1e-3,\n",
    "    'n_epoch': 10000,\n",
    "    'alpha': 1.0,\n",
    "    'batchsize': 10000,\n",
    "    'timestep': 201,\n",
    "    'timeslice_hops': 0,\n",
    "    'timeslice_steps': 1,\n",
    "    'n_data_per_batch': 1,\n",
    "    'forward_sequence':False,\n",
    "    'criterion': LpLoss(),\n",
    "    'plot': False\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = OneDDatasetBuilder(\n",
    "#     raw_dir='/data1/tam/datasets',\n",
    "#     root_dir='/data1/tam/downloaded_datasets_edge_node_separated',\n",
    "#     sub_dir='processed',\n",
    "#     subjects='all',\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)],\n",
    "#     data_type = torch.float32,\n",
    "#     readme='edge_index(2xn_edge), node_attr(n_nodex10), pressure+flowrate(n_nodex201)'\n",
    "# )\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_edge_node_separated',\n",
    "    sub_dir='processed',\n",
    "    subjects='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)],\n",
    "    data_type = torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = normalize(\n",
    "#     dataset=dataset,\n",
    "#     sub_dir='normalized',\n",
    "#     scaler_dict={\n",
    "#         'node_attr': ('minmax_scaler', 0, None),\n",
    "#         'edge_attr': ('minmax_scaler', 0, None),\n",
    "#         'pressure': ('minmax_scaler', None, None),\n",
    "#         # 'flowrate': ('robust_scaler', None, None),\n",
    "#         # 'pressure_dot': ('minmax_scaler', None, None),\n",
    "#         # 'flowrate_dot': ('robust_scaler', None, None),\n",
    "#         # 'time': ('minmax_scaler', None, None)\n",
    "#     }\n",
    "# )\n",
    "dataset = OneDDatasetLoader(\n",
    "    root_dir='/data1/tam/downloaded_datasets_edge_node_separated',\n",
    "    sub_dir='normalized',\n",
    "    subjects='all',\n",
    "    time_names=[str(i).zfill(3) for i in range(201)],\n",
    "    data_type = torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = batchgraph_generation_wise(\n",
    "#     dataset,\n",
    "#     sub_dir='batched',\n",
    "#     batch_gens=[[14,15]],\n",
    "#     timestep=args.timestep,\n",
    "#     timeslice_hops=args.timeslice_hops,\n",
    "#     timeslice_steps=args.timeslice_steps\n",
    "# )\n",
    "# dataset = batchgraph(\n",
    "#     dataset,\n",
    "#     sub_dir='batched_1',\n",
    "#     batchsize=None,\n",
    "#     timestep=args.timestep,\n",
    "#     timeslice_hops=args.timeslice_hops,\n",
    "#     timeslice_steps=args.timeslice_steps\n",
    "# )\n",
    "# dataset = OneDDatasetLoader(\n",
    "#     root_dir='/data1/tam/downloaded_datasets_Static_v1',\n",
    "#     sub_dir='batched_1',\n",
    "#     subjects='all',\n",
    "#     time_names=[str(i).zfill(3) for i in range(201)],\n",
    "#     data_type = torch.float32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_size = 2\n",
    "# fold = [list(range(i*fold_size, (i+1)*fold_size)) for i in range(8)]\n",
    "# n_train_fold = 5\n",
    "\n",
    "# for i in range(n_train_fold):\n",
    "#     test_subset = fold[i]\n",
    "#     train_subset = []\n",
    "#     for j in range(n_train_fold):\n",
    "#         if j != i:\n",
    "#             train_subset += fold[j]\n",
    "\n",
    "# (train_loader, test_loader) = dataset_to_loader(\n",
    "#     dataset=dataset,\n",
    "#     data_subset_dict={\n",
    "#         'train': list(range(5, 6)),\n",
    "#         'test': list(range(0, 1))\n",
    "#     },\n",
    "#     n_data_per_batch=args.n_data_per_batch\n",
    "# )\n",
    "\n",
    "# train_set, test_set = dataset_to_loader(\n",
    "#     dataset=dataset,\n",
    "#     data_subset_dict={\n",
    "#         'train': list(range(6, 36)),\n",
    "#         'test': list(range(0, 5))\n",
    "#     },\n",
    "#     n_data_per_batch=args.n_data_per_batch\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initializing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentFormulationNet(\n",
    "    n_field=args.n_field,\n",
    "    n_meshfield=args.n_meshfield,\n",
    "    hidden_size=args.hidden_size,\n",
    "    n_hidden=args.n_hidden,\n",
    "    act=args.act,\n",
    "    dropout=args.dropout,\n",
    "    use_hidden=args.use_hidden\n",
    ")\n",
    "setattr(model, 'name', 'model_GCN')\n",
    "model = model.to(args.device)\n",
    "# model.load_state_dict(torch.load(f'models/{model.name}_node2_epoch200.pth', map_location=args.device) )\n",
    "# optimizer1 = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "# optimizer2 = torch.optim.LBFGS(model.parameters(), lr=args.lr)\n",
    "# setattr(args, 'optimizer', optimizer2)\n",
    "# setattr(args, 'optimizer2', optimizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, args):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    ##\n",
    "    F_true = torch.cat([data.pressure.unsqueeze(2)], dim=2) \\\n",
    "                .float().to(args.device)\n",
    "    ##\n",
    "    if not args.forward_sequence:\n",
    "        F_0 = F_true[:,0,:]\n",
    "    else:\n",
    "        F_0 = F_true[:,:-1,:]\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "    # edge_index = torch.cat([data.edge_index, torch.flip(data.edge_index, dims=[0])], dim=1).to(args.device)\n",
    "    meshfield = (data.node_attr.float().to(args.device), data.edge_attr.float().to(args.device))\n",
    "    # meshfield = (data.node_attr.float().to(args.device), torch.cat([data.edge_attr, data.edge_attr], dim=0).float().to(args.device))\n",
    "    F_true = F_true[:,1:args.n_time,:]\n",
    "    ##\n",
    "    F_pred = model.forward(\n",
    "        F_0=F_0,\n",
    "        edge_index=edge_index,\n",
    "        meshfield=meshfield,\n",
    "        n_time=args.n_time-1\n",
    "    )\n",
    "    ##\n",
    "    optimizer.zero_grad()\n",
    "    model.train()\n",
    "    loss = args.criterion(F_pred, F_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def eval(model, data, args):\n",
    "    ##\n",
    "    F_true = torch.cat([data.pressure.unsqueeze(2)], dim=2) \\\n",
    "                .float().to(args.device)\n",
    "    ##\n",
    "    if not args.forward_sequence:\n",
    "        F_0 = F_true[:,0,:]\n",
    "    else:\n",
    "        F_0 = F_true[:,:-1,:]\n",
    "    edge_index = data.edge_index.to(args.device)\n",
    "    # edge_index = torch.cat([data.edge_index, torch.flip(data.edge_index, dims=[0])], dim=1).to(args.device)\n",
    "    meshfield = (data.node_attr.float().to(args.device), data.edge_attr.float().to(args.device))\n",
    "    # meshfield = (data.node_attr.float().to(args.device), torch.cat([data.edge_attr, data.edge_attr], dim=0).float().to(args.device))\n",
    "    F_true = F_true[:,1:args.n_time,:]\n",
    "    ##\n",
    "    with torch.no_grad():\n",
    "        F_pred = model.forward(\n",
    "            F_0=F_0,\n",
    "            edge_index=edge_index,\n",
    "            meshfield=meshfield,\n",
    "            n_time=args.n_time-1\n",
    "        )\n",
    "    ##\n",
    "    loss = args.criterion(F_pred, F_true)\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = list(range(0, 10))\n",
    "test_subset = list(range(20,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam/.conda/envs/tam_geometric/lib/python3.11/site-packages/torch_geometric/utils/sparse.py:268: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  adj = torch.sparse_csr_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 2.1503426109751076; eval loss = 0.33225364956472586\n",
      "Epoch 1: train loss = 0.4484237944707274; eval loss = 0.3209211368424196\n",
      "Epoch 2: train loss = 0.42690608945364755; eval loss = 0.5131567030524212\n",
      "Epoch 3: train loss = 0.39460279063011194; eval loss = 0.2183445394039155\n",
      "Epoch 4: train loss = 0.38716680835932493; eval loss = 0.21350127811144498\n",
      "Epoch 5: train loss = 0.3806394819791119; eval loss = 0.20399870115908822\n",
      "Epoch 6: train loss = 0.3816278555120031; eval loss = 0.32272220325345763\n",
      "Epoch 7: train loss = 0.3716800686282416; eval loss = 0.49275097211024593\n",
      "Epoch 8: train loss = 0.3698796476237477; eval loss = 0.46380399912595743\n",
      "Epoch 9: train loss = 0.36606555556257564; eval loss = 0.4854874399801096\n",
      "Epoch 10: train loss = 0.3683885036346813; eval loss = 0.5005667395889759\n",
      "Epoch 11: train loss = 0.3683725016502043; eval loss = 0.3015189155315357\n",
      "Epoch 12: train loss = 0.3639293443411588; eval loss = 0.42723002936691035\n",
      "Epoch 13: train loss = 0.35401408514007937; eval loss = 0.45453479792922724\n",
      "Epoch 14: train loss = 0.35067858950545394; eval loss = 0.43799839448183786\n",
      "Epoch 15: train loss = 0.34196361231928063; eval loss = 0.3780793563152353\n",
      "Epoch 16: train loss = 0.34178927727043623; eval loss = 0.40957379806786776\n",
      "Epoch 17: train loss = 0.35209205591430265; eval loss = 0.41641771886497714\n",
      "Epoch 18: train loss = 0.3553432811362047; eval loss = 0.2883430427561204\n",
      "Epoch 19: train loss = 0.3493323326110839; eval loss = 0.26002760231494904\n",
      "Epoch 20: train loss = 0.3504994207372267; eval loss = 0.2428034101612866\n",
      "Epoch 21: train loss = 0.34859471550832194; eval loss = 0.24638049397617556\n",
      "Epoch 22: train loss = 0.34738616008932394; eval loss = 0.2511853105388581\n",
      "Epoch 23: train loss = 0.3464165364081662; eval loss = 0.253808207033823\n",
      "Epoch 24: train loss = 0.3476054828303556; eval loss = 0.25052734278142424\n",
      "Epoch 25: train loss = 0.3470859903221329; eval loss = 0.2516891714185475\n",
      "Epoch 26: train loss = 0.3432106588346263; eval loss = 0.3730771467089651\n",
      "Epoch 27: train loss = 0.37040257360786194; eval loss = 0.3466748169490269\n",
      "Epoch 28: train loss = 0.3635065502797565; eval loss = 0.2958873715251685\n",
      "Epoch 29: train loss = 0.33941850671544677; eval loss = 0.4728178735822439\n",
      "Epoch 30: train loss = 0.3451865498597423; eval loss = 0.35253947414457815\n",
      "Epoch 31: train loss = 0.3376415620247523; eval loss = 0.36481957091018574\n",
      "Epoch 32: train loss = 0.33537409175187344; eval loss = 0.36504512326791894\n",
      "Epoch 33: train loss = 0.33449858194217097; eval loss = 0.3790631340816617\n",
      "Epoch 34: train loss = 0.33415598856906087; eval loss = 0.3844750262796877\n",
      "Epoch 35: train loss = 0.333885610025997; eval loss = 0.23201133310794828\n",
      "Epoch 36: train loss = 0.3341489834710955; eval loss = 0.21090118493884794\n",
      "Epoch 37: train loss = 0.3364032277216514; eval loss = 0.5723774544894695\n",
      "Epoch 38: train loss = 0.3375296372299394; eval loss = 0.41436925716698136\n",
      "Epoch 39: train loss = 0.3260808993751804; eval loss = 0.47082343697547907\n",
      "Epoch 40: train loss = 0.33886961794147885; eval loss = 0.47763475403189654\n",
      "Epoch 41: train loss = 0.3362131767595807; eval loss = 0.48685356868164886\n",
      "Epoch 42: train loss = 0.3337009480843941; eval loss = 0.48538740910589706\n",
      "Epoch 43: train loss = 0.3309780365477006; eval loss = 0.42288091592490673\n",
      "Epoch 44: train loss = 0.3334451233968138; eval loss = 0.21381601877510556\n",
      "Epoch 45: train loss = 0.3266287835625312; eval loss = 0.21524969162419433\n",
      "Epoch 46: train loss = 0.3229033964841317; eval loss = 0.2424025833606719\n",
      "Epoch 47: train loss = 0.3364545980778833; eval loss = 0.2256285436451436\n",
      "Epoch 48: train loss = 0.33077839692123245; eval loss = 0.9017648473381994\n",
      "Epoch 49: train loss = 0.3356141373515128; eval loss = 0.39320534095168114\n",
      "Epoch 50: train loss = 0.3219086968650421; eval loss = 0.21540796430781478\n",
      "Epoch 51: train loss = 0.32686152200525015; eval loss = 0.213566225487739\n",
      "Epoch 52: train loss = 0.3260680350164572; eval loss = 0.20193416066467773\n",
      "Epoch 53: train loss = 0.3245925474911928; eval loss = 0.243578530382365\n",
      "Epoch 54: train loss = 0.3279709337900082; eval loss = 0.2418885212391613\n",
      "Epoch 55: train loss = 0.3254658496007323; eval loss = 0.372718377038836\n",
      "Epoch 56: train loss = 0.319353616485993; eval loss = 0.656677942723036\n",
      "Epoch 57: train loss = 0.3330351694797476; eval loss = 0.49015479348599933\n",
      "Epoch 58: train loss = 0.3258601364990075; eval loss = 0.3287946768105032\n",
      "Epoch 59: train loss = 0.3295043488033116; eval loss = 0.2590981588388482\n",
      "Epoch 60: train loss = 0.3239651530360183; eval loss = 0.23374301381409174\n",
      "Epoch 61: train loss = 0.3190395855344832; eval loss = 0.5868931785225865\n",
      "Epoch 62: train loss = 0.33379139572692423; eval loss = 0.3584908302873372\n",
      "Epoch 63: train loss = 0.3310417037767668; eval loss = 0.3908695904538038\n",
      "Epoch 64: train loss = 0.3184871980920434; eval loss = 0.3798159332945947\n",
      "Epoch 65: train loss = 0.3185054232987264; eval loss = 0.38351550977677107\n",
      "Epoch 66: train loss = 0.3233271628463019; eval loss = 0.33878082130104276\n",
      "Epoch 67: train loss = 0.32020159934957815; eval loss = 0.36585682537406683\n",
      "Epoch 68: train loss = 0.31649213672305143; eval loss = 0.3796471739187838\n",
      "Epoch 69: train loss = 0.3172050386977693; eval loss = 0.37756209261715423\n",
      "Epoch 70: train loss = 0.31786765794580185; eval loss = 0.3742791647091507\n",
      "Epoch 71: train loss = 0.3167032042207817; eval loss = 0.39373469725251226\n",
      "Epoch 72: train loss = 0.33046764818330604; eval loss = 0.48950382694602\n",
      "Epoch 73: train loss = 0.33097281757121283; eval loss = 0.3413690589368341\n",
      "Epoch 74: train loss = 0.3205224981841942; eval loss = 0.326838856562972\n",
      "Epoch 75: train loss = 0.3200234821997583; eval loss = 0.31272779777646087\n",
      "Epoch 76: train loss = 0.320267357553045; eval loss = 0.2968202754855157\n",
      "Epoch 77: train loss = 0.3206789502874016; eval loss = 0.29811677150428306\n",
      "Epoch 78: train loss = 0.32127832388505345; eval loss = 0.26910705585032685\n",
      "Epoch 79: train loss = 0.32153042141969007; eval loss = 0.24926205817610017\n",
      "Epoch 80: train loss = 0.32408759603276854; eval loss = 0.24470062553882588\n",
      "Epoch 81: train loss = 0.3260821712513764; eval loss = 0.20421464322134866\n",
      "Epoch 82: train loss = 0.3203633377949396; eval loss = 0.27689260616898526\n",
      "Epoch 83: train loss = 0.3242873237468303; eval loss = 0.28128831274807453\n",
      "Epoch 84: train loss = 0.3176216523473462; eval loss = 0.4449845459312203\n",
      "Epoch 85: train loss = 0.32274510447556787; eval loss = 0.2916911281645299\n",
      "Epoch 86: train loss = 0.31837853106359637; eval loss = 0.592902820557356\n",
      "Epoch 87: train loss = 0.32412522130956245; eval loss = 0.5030414983630183\n",
      "Epoch 88: train loss = 0.33192005148157483; eval loss = 0.2604335583746432\n",
      "Epoch 89: train loss = 0.3321896150397757; eval loss = 0.22852543368935593\n",
      "Epoch 90: train loss = 0.3231978455247979; eval loss = 0.2371269688010216\n",
      "Epoch 91: train loss = 0.3244630914802353; eval loss = 0.29703578352928156\n",
      "Epoch 92: train loss = 0.3121478408575059; eval loss = 0.6639511622488504\n",
      "Epoch 93: train loss = 0.31822078434440004; eval loss = 0.3873736094683409\n",
      "Epoch 94: train loss = 0.3274535767268389; eval loss = 0.2674711802974342\n",
      "Epoch 95: train loss = 0.3287286070796351; eval loss = 0.21306377463042728\n",
      "Epoch 96: train loss = 0.32107552016774815; eval loss = 0.19709522183984496\n",
      "Epoch 97: train loss = 0.3204414158438644; eval loss = 0.39409605041146306\n",
      "Epoch 98: train loss = 0.3391601185624798; eval loss = 0.3083349665006001\n",
      "Epoch 99: train loss = 0.343363635862867; eval loss = 0.16223904956132176\n",
      "Epoch 100: train loss = 0.34710298695911973; eval loss = 0.2752068880945443\n",
      "Epoch 101: train loss = 0.34230405821775395; eval loss = 0.27348597176993883\n",
      "Epoch 102: train loss = 0.34421119124939037; eval loss = 0.25803099945187585\n",
      "Epoch 103: train loss = 0.3473721633975705; eval loss = 0.17639094777405268\n",
      "Epoch 104: train loss = 0.3504726615113518; eval loss = 0.16508217621594656\n",
      "Epoch 105: train loss = 0.34747023507952685; eval loss = 0.48403026536107063\n",
      "Epoch 106: train loss = 0.36731371485317743; eval loss = 0.6611768864095209\n",
      "Epoch 107: train loss = 0.377106540525953; eval loss = 0.5185355525463822\n",
      "Epoch 108: train loss = 0.3716253122935693; eval loss = 0.3254058190310994\n",
      "Epoch 109: train loss = 0.35361140485232084; eval loss = 0.5574858197942375\n",
      "Epoch 110: train loss = 0.3639030260965227; eval loss = 0.6632510125637054\n",
      "Epoch 111: train loss = 0.38291344124202925; eval loss = 0.586228564381599\n",
      "Epoch 112: train loss = 0.36757593167324865; eval loss = 0.6881688311696057\n",
      "Epoch 113: train loss = 0.37964908281962084; eval loss = 0.287490609101951\n",
      "Epoch 114: train loss = 0.3639246119807164; eval loss = 0.42796578630805016\n",
      "Epoch 115: train loss = 0.35674116356919217; eval loss = 0.4551514517515897\n",
      "Epoch 116: train loss = 0.3470154072468479; eval loss = 0.3403131943196056\n",
      "Epoch 117: train loss = 0.3568081056388717; eval loss = 0.45097805559635196\n",
      "Epoch 118: train loss = 0.3674799506552517; eval loss = 0.4648945148413383\n",
      "Epoch 119: train loss = 0.3768891125606994; eval loss = 0.5396520923823118\n",
      "Epoch 120: train loss = 0.36452781657377886; eval loss = 0.5328920288011434\n",
      "Epoch 121: train loss = 0.3488850866754851; eval loss = 0.3269735919311642\n",
      "Epoch 122: train loss = 0.3463299926370383; eval loss = 0.35298287061353534\n",
      "Epoch 123: train loss = 0.3746171284777421; eval loss = 0.2877847710624337\n",
      "Epoch 124: train loss = 0.34577153312663234; eval loss = 0.3633855395019055\n",
      "Epoch 125: train loss = 0.3578294720500707; eval loss = 0.26134126633405674\n",
      "Epoch 126: train loss = 0.35084749152883876; eval loss = 0.2710660930412511\n",
      "Epoch 127: train loss = 0.351179264175395; eval loss = 0.2797516025602817\n",
      "Epoch 128: train loss = 0.35124504659324896; eval loss = 0.2840146161615849\n",
      "Epoch 129: train loss = 0.3506604352345069; eval loss = 0.27654332326104214\n",
      "Epoch 130: train loss = 0.35307034819076466; eval loss = 0.49827289395034324\n",
      "Epoch 131: train loss = 0.35096815461292874; eval loss = 0.225347831047007\n",
      "Epoch 132: train loss = 0.3489640355110168; eval loss = 0.23156258525947734\n",
      "Epoch 133: train loss = 0.3433816153556108; eval loss = 0.2388640521094202\n",
      "Epoch 134: train loss = 0.3483490588453909; eval loss = 0.7256983444094662\n",
      "Epoch 135: train loss = 0.3705205235940714; eval loss = 0.6846294409284994\n",
      "Epoch 136: train loss = 0.3584450748749076; eval loss = 0.36655750001470244\n",
      "Epoch 137: train loss = 0.36681844185416895; eval loss = 0.2862868965603409\n",
      "Epoch 138: train loss = 0.35949031682685006; eval loss = 0.29081313746670884\n",
      "Epoch 139: train loss = 0.3431875744524102; eval loss = 0.3802163833752272\n",
      "Epoch 140: train loss = 0.35417059824491537; eval loss = 0.34820686094462866\n",
      "Epoch 141: train loss = 0.34981883969157945; eval loss = 0.33960438147187205\n",
      "Epoch 142: train loss = 0.35303948974857735; eval loss = 0.4809868130832908\n",
      "Epoch 143: train loss = 0.39023192273452884; eval loss = 0.43973579443991206\n",
      "Epoch 144: train loss = 0.35368253011256473; eval loss = 0.3669807687401771\n",
      "Epoch 145: train loss = 0.35672861586014437; eval loss = 0.3163753841072323\n",
      "Epoch 146: train loss = 0.36016727626944584; eval loss = 0.30172698944807064\n",
      "Epoch 147: train loss = 0.3655473866189519; eval loss = 0.23743668974687662\n",
      "Epoch 148: train loss = 0.35922587631891184; eval loss = 0.23838128149509435\n",
      "Epoch 149: train loss = 0.3587081652755538; eval loss = 0.2681948374956845\n",
      "Epoch 150: train loss = 0.3711349343260129; eval loss = 0.2221698866536218\n",
      "Epoch 151: train loss = 0.35637616210927575; eval loss = 0.3440319607034325\n",
      "Epoch 152: train loss = 0.37384325110663985; eval loss = 0.3771185927713912\n",
      "Epoch 153: train loss = 0.3981850622221828; eval loss = 0.3422197004159291\n",
      "Epoch 154: train loss = 0.3582070702686904; eval loss = 0.28426579882701225\n",
      "Epoch 155: train loss = 0.35555380877728254; eval loss = 0.2732130627458293\n",
      "Epoch 156: train loss = 0.35306223652636; eval loss = 0.26008822349831473\n",
      "Epoch 157: train loss = 0.34521293158953387; eval loss = 0.26523292126754927\n",
      "Epoch 158: train loss = 0.35911164029190934; eval loss = 0.36392413266003126\n",
      "Epoch 159: train loss = 0.3518220642581581; eval loss = 0.2710929404323303\n",
      "Epoch 160: train loss = 0.3496420974843204; eval loss = 0.27379783522337675\n",
      "Epoch 161: train loss = 0.35023365650946897; eval loss = 0.31568524986505486\n",
      "Epoch 162: train loss = 0.3498362093232573; eval loss = 0.4624386373907327\n",
      "Epoch 163: train loss = 0.37411705420042063; eval loss = 0.38249982520937925\n",
      "Epoch 164: train loss = 0.37115471623837953; eval loss = 0.28050516359508043\n",
      "Epoch 165: train loss = 0.3572865013654034; eval loss = 0.4812308860321841\n",
      "Epoch 166: train loss = 0.3603148103381195; eval loss = 0.5756749249994754\n",
      "Epoch 167: train loss = 0.37325963797047734; eval loss = 0.5634148145715391\n",
      "Epoch 168: train loss = 0.35620039825638133; eval loss = 0.6417023781687019\n",
      "Epoch 169: train loss = 0.3532554457585018; eval loss = 0.33970789145678254\n",
      "Epoch 170: train loss = 0.3532893209097287; eval loss = 0.5295179747045045\n",
      "Epoch 171: train loss = 0.33684985184421146; eval loss = 0.6055864244699478\n",
      "Epoch 172: train loss = 0.3694229729784033; eval loss = 0.563462878577411\n",
      "Epoch 173: train loss = 0.35016337789905566; eval loss = 0.5115216532722114\n",
      "Epoch 174: train loss = 0.33720726333558565; eval loss = 0.3887806730344892\n",
      "Epoch 175: train loss = 0.32989529861758154; eval loss = 0.4424897534772756\n",
      "Epoch 176: train loss = 0.3505793154860536; eval loss = 0.44894929230213193\n",
      "Epoch 177: train loss = 0.33085077131787943; eval loss = 0.7464296370744707\n",
      "Epoch 178: train loss = 0.3636879534460604; eval loss = 0.43962430177877343\n",
      "Epoch 179: train loss = 0.3262305990792811; eval loss = 0.7077428642660378\n",
      "Epoch 180: train loss = 0.362465789852043; eval loss = 0.5433974464734399\n",
      "Epoch 181: train loss = 0.3552974349198242; eval loss = 0.4444809425622221\n",
      "Epoch 182: train loss = 0.32624991843476886; eval loss = 0.4674564764524499\n",
      "Epoch 183: train loss = 0.3234044952938954; eval loss = 0.7048747465014463\n",
      "Epoch 184: train loss = 0.3494491389331719; eval loss = 0.43415882345288975\n",
      "Epoch 185: train loss = 0.32889349060133105; eval loss = 0.37378624267876126\n",
      "Epoch 186: train loss = 0.32932472632577026; eval loss = 0.3843670189380644\n",
      "Epoch 187: train loss = 0.3288499158807099; eval loss = 0.3806977113708854\n",
      "Epoch 188: train loss = 0.3125716517679394; eval loss = 0.482721215424438\n",
      "Epoch 189: train loss = 0.3267983210583527; eval loss = 0.4354409463703634\n",
      "Epoch 190: train loss = 0.3472568513825535; eval loss = 0.4235503971576688\n",
      "Epoch 191: train loss = 0.3408897413561742; eval loss = 0.42318939324468346\n",
      "Epoch 192: train loss = 0.34731339321782184; eval loss = 0.40433338005095704\n",
      "Epoch 193: train loss = 0.3483577120738725; eval loss = 0.4161664508283137\n",
      "Epoch 194: train loss = 0.3379105352796615; eval loss = 0.4328108951449395\n",
      "Epoch 195: train loss = 0.32256372629975283; eval loss = 0.5784469867746035\n",
      "Epoch 196: train loss = 0.33144779798264307; eval loss = 0.47377882059663556\n",
      "Epoch 197: train loss = 0.31348122842609877; eval loss = 0.626093603670597\n",
      "Epoch 198: train loss = 0.34004441027839966; eval loss = 0.5306855896487832\n",
      "Epoch 199: train loss = 0.3348979159879187; eval loss = 0.4474490871652957\n",
      "Epoch 200: train loss = 0.34885657951235766; eval loss = 0.5216174597541489\n",
      "Epoch 201: train loss = 0.3706927102369567; eval loss = 0.5836737211793659\n",
      "Epoch 202: train loss = 0.3772013816051185; eval loss = 0.3848669361323117\n",
      "Epoch 203: train loss = 0.3641954703877371; eval loss = 0.4119464922696349\n",
      "Epoch 204: train loss = 0.36485398778071004; eval loss = 0.3961192425340412\n",
      "Epoch 205: train loss = 0.3672907763781649; eval loss = 0.4016928132623432\n",
      "Epoch 206: train loss = 0.3659182270057499; eval loss = 0.43407433883597424\n",
      "Epoch 207: train loss = 0.366025521264722; eval loss = 0.4081457480788232\n",
      "Epoch 208: train loss = 0.36209261479477084; eval loss = 0.5483626173809172\n",
      "Epoch 209: train loss = 0.3688575668881337; eval loss = 0.34985838985691436\n",
      "Epoch 210: train loss = 0.3412866115880508; eval loss = 0.33589893051733577\n",
      "Epoch 211: train loss = 0.33752763131633395; eval loss = 0.3495787996798754\n",
      "Epoch 212: train loss = 0.333799100170533; eval loss = 0.35284876978645746\n",
      "Epoch 213: train loss = 0.3334292561436692; eval loss = 0.35311589452127623\n",
      "Epoch 214: train loss = 0.33150686727215845; eval loss = 0.35483216214925034\n",
      "Epoch 215: train loss = 0.3318499027130504; eval loss = 0.3568622522676984\n",
      "Epoch 216: train loss = 0.3306525011236468; eval loss = 0.3426197273656725\n",
      "Epoch 217: train loss = 0.3325024750083685; eval loss = 0.35314599105289984\n",
      "Epoch 218: train loss = 0.32704560582836467; eval loss = 0.3533957377076151\n",
      "Epoch 219: train loss = 0.32874386385083204; eval loss = 0.3647471815347671\n",
      "Epoch 220: train loss = 0.32991170929744845; eval loss = 0.5139717366546396\n",
      "Epoch 221: train loss = 0.3577484083361923; eval loss = 0.6469275634735823\n",
      "Epoch 222: train loss = 0.3652781026127438; eval loss = 0.42735104262828827\n",
      "Epoch 223: train loss = 0.3337251547103127; eval loss = 0.4086109277393138\n",
      "Epoch 224: train loss = 0.33839957571278; eval loss = 0.40226018428802474\n",
      "Epoch 225: train loss = 0.3414410888217389; eval loss = 0.4079493535682557\n",
      "Epoch 226: train loss = 0.3411881856930754; eval loss = 0.3889170400798321\n",
      "Epoch 227: train loss = 0.33941770562281215; eval loss = 0.3938307929784058\n",
      "Epoch 228: train loss = 0.33656060105810565; eval loss = 0.3891804059967402\n",
      "Epoch 229: train loss = 0.33252758548284556; eval loss = 0.3932494027540086\n",
      "Epoch 230: train loss = 0.332692809868604; eval loss = 0.8190580457448948\n",
      "Epoch 231: train loss = 0.33960436781247455; eval loss = 0.38843942526727926\n",
      "Epoch 232: train loss = 0.33075792140637833; eval loss = 0.3905447047824657\n",
      "Epoch 233: train loss = 0.33536340963716316; eval loss = 0.80365139991045\n",
      "Epoch 234: train loss = 0.367924441738675; eval loss = 0.373820211738348\n",
      "Epoch 235: train loss = 0.32836246397346264; eval loss = 0.37985069863498194\n",
      "Epoch 236: train loss = 0.32433784380555164; eval loss = 0.3794861510396006\n",
      "Epoch 237: train loss = 0.3248391565866767; eval loss = 0.37144588865339734\n",
      "Epoch 238: train loss = 0.32544986189653485; eval loss = 0.3647787815758159\n",
      "Epoch 239: train loss = 0.3234142288565637; eval loss = 0.3615201599895954\n",
      "Epoch 240: train loss = 0.32313073566183453; eval loss = 0.3595317021633192\n",
      "Epoch 241: train loss = 0.32144074204067385; eval loss = 0.35748680308461184\n",
      "Epoch 242: train loss = 0.3210412644160291; eval loss = 0.3556509837508204\n",
      "Epoch 243: train loss = 0.31974810237685847; eval loss = 0.3533157110214234\n",
      "Epoch 244: train loss = 0.3205604958347976; eval loss = 0.34946928732097127\n",
      "Epoch 245: train loss = 0.3344815808037917; eval loss = 0.542312820841159\n",
      "Epoch 246: train loss = 0.35476882336661214; eval loss = 0.5719134807586668\n",
      "Epoch 247: train loss = 0.3282365257230896; eval loss = 0.5157387234891452\n",
      "Epoch 248: train loss = 0.32498272104809683; eval loss = 0.801243770867586\n",
      "Epoch 249: train loss = 0.3495382932014763; eval loss = 0.7526648007333276\n",
      "Epoch 250: train loss = 0.373823355572919; eval loss = 0.5490187052637334\n",
      "Epoch 251: train loss = 0.3555781731071572; eval loss = 0.5122029004795922\n",
      "Epoch 252: train loss = 0.35110866806159413; eval loss = 0.768054108135402\n",
      "Epoch 253: train loss = 0.3673983357536296; eval loss = 0.4992102095857261\n",
      "Epoch 254: train loss = 0.3560267255331078; eval loss = 0.8012428600341082\n",
      "Epoch 255: train loss = 0.3648626528059443; eval loss = 0.465737709465126\n",
      "Epoch 256: train loss = 0.3643343260822197; eval loss = 0.45637696608900996\n",
      "Epoch 257: train loss = 0.35409009254847956; eval loss = 0.4597836080938579\n",
      "Epoch 258: train loss = 0.34038437328611815; eval loss = 0.5083940746262668\n",
      "Epoch 259: train loss = 0.36162640526890755; eval loss = 0.49742956738919014\n",
      "Epoch 260: train loss = 0.3562394219140212; eval loss = 0.750506757758558\n",
      "Epoch 261: train loss = 0.3710176683962345; eval loss = 0.49985530134290446\n",
      "Epoch 262: train loss = 0.3596466590339938; eval loss = 0.48748003691434894\n",
      "Epoch 263: train loss = 0.3558670561760663; eval loss = 0.8405118957161903\n",
      "Epoch 264: train loss = 0.3824085639789702; eval loss = 0.4666473325341939\n",
      "Epoch 265: train loss = 0.32695988084500033; eval loss = 0.6604145004724467\n",
      "Epoch 266: train loss = 0.3531084259351094; eval loss = 0.5297246444970369\n",
      "Epoch 267: train loss = 0.34316476915652566; eval loss = 0.7346836285044752\n",
      "Epoch 268: train loss = 0.3524735543566446; eval loss = 0.7145112846046684\n",
      "Epoch 269: train loss = 0.360820377866427; eval loss = 0.5385419316589832\n",
      "Epoch 270: train loss = 0.34664230064178503; eval loss = 0.498596217483282\n",
      "Epoch 271: train loss = 0.33699937475224356; eval loss = 0.48137989733368164\n",
      "Epoch 272: train loss = 0.3409543636565407; eval loss = 0.47489466145634684\n",
      "Epoch 273: train loss = 0.3585476668862005; eval loss = 0.4775867400070033\n",
      "Epoch 274: train loss = 0.3436697397070626; eval loss = 0.4793132115155459\n",
      "Epoch 275: train loss = 0.3452907454532882; eval loss = 0.48186202906072145\n",
      "Epoch 276: train loss = 0.33724336667607224; eval loss = 0.692066710442305\n",
      "Epoch 277: train loss = 0.37094410151864093; eval loss = 0.6214356143027545\n",
      "Epoch 278: train loss = 0.3751365309581161; eval loss = 0.6615731269121169\n",
      "Epoch 279: train loss = 0.37496223704268533; eval loss = 0.44911932945251465\n",
      "Epoch 280: train loss = 0.35361696283022553; eval loss = 0.6164825865998866\n",
      "Epoch 281: train loss = 0.3979649158815542; eval loss = 0.47826757949466536\n",
      "Epoch 282: train loss = 0.34969909970338137; eval loss = 0.4922043333450952\n",
      "Epoch 283: train loss = 0.3345082251665492; eval loss = 0.5629802805682025\n",
      "Epoch 284: train loss = 0.3374993385126194; eval loss = 0.8542602965608241\n",
      "Epoch 285: train loss = 0.36445645429193974; eval loss = 0.6088692918419839\n",
      "Epoch 286: train loss = 0.3764674163733919; eval loss = 0.5819289963692427\n",
      "Epoch 287: train loss = 0.37439265102148056; eval loss = 0.5988760814070699\n",
      "Epoch 288: train loss = 0.37503733066841954; eval loss = 0.6440797336399555\n",
      "Epoch 289: train loss = 0.36658150733759; eval loss = 0.5369208557531239\n",
      "Epoch 290: train loss = 0.3769003953784703; eval loss = 0.5531063089147213\n",
      "Epoch 291: train loss = 0.36282066131631535; eval loss = 0.5498631866648789\n",
      "Epoch 292: train loss = 0.36706440771619475; eval loss = 0.5805386025458572\n",
      "Epoch 293: train loss = 0.3558830851688982; eval loss = 0.7287600301206109\n",
      "Epoch 294: train loss = 0.36006604290256894; eval loss = 0.6628735214471815\n",
      "Epoch 295: train loss = 0.3911721819701294; eval loss = 0.4992100875824691\n",
      "Epoch 296: train loss = 0.34363195843373734; eval loss = 0.5518964305520058\n",
      "Epoch 297: train loss = 0.36903199258570857; eval loss = 0.5273822285234926\n",
      "Epoch 298: train loss = 0.3667977917939424; eval loss = 0.5072378218173977\n",
      "Epoch 299: train loss = 0.36972654440129793; eval loss = 0.5871395524591205\n",
      "Epoch 300: train loss = 0.5071278981243571; eval loss = 0.5972019229084256\n",
      "Epoch 301: train loss = 0.4905129889957607; eval loss = 0.6455425526946782\n",
      "Epoch 302: train loss = 0.4848025450482965; eval loss = 0.6932097778966027\n",
      "Epoch 303: train loss = 0.48181498733659567; eval loss = 0.7453474371383592\n",
      "Epoch 304: train loss = 0.5069951484911144; eval loss = 0.7247808035463097\n",
      "Epoch 305: train loss = 0.5193933566721777; eval loss = 0.7276168148964647\n",
      "Epoch 306: train loss = 0.5096569494344295; eval loss = 0.9131334014236923\n",
      "Epoch 307: train loss = 0.5206056024568775; eval loss = 0.8899490013718603\n",
      "Epoch 308: train loss = 0.508754764838765; eval loss = 0.8908972889184951\n",
      "Epoch 309: train loss = 0.5049386549120146; eval loss = 0.8741229393829908\n",
      "Epoch 310: train loss = 0.5109109911136329; eval loss = 0.6848274264484641\n",
      "Epoch 311: train loss = 0.46746998606249696; eval loss = 0.9398740306496621\n",
      "Epoch 312: train loss = 0.5085103282084068; eval loss = 0.7118614949285986\n",
      "Epoch 313: train loss = 0.48685591869677114; eval loss = 0.6990222260355952\n",
      "Epoch 314: train loss = 0.4601764633941153; eval loss = 0.8990537524223324\n",
      "Epoch 315: train loss = 0.4975159061141313; eval loss = 0.6833707193533581\n",
      "Epoch 316: train loss = 0.4828501769031089; eval loss = 0.6884256917983295\n",
      "Epoch 317: train loss = 0.5112582035362722; eval loss = 0.697393278901776\n",
      "Epoch 318: train loss = 0.49844374942282843; eval loss = 0.8081121556460856\n",
      "Epoch 319: train loss = 0.48612087111299246; eval loss = 0.7869890630245207\n",
      "Epoch 320: train loss = 0.5077046964628003; eval loss = 0.7775087077170613\n",
      "Epoch 321: train loss = 0.47860117380817724; eval loss = 0.7930604576000146\n",
      "Epoch 322: train loss = 0.4784038150683045; eval loss = 0.7944372830291588\n",
      "Epoch 323: train loss = 0.4779200550789633; eval loss = 0.9378719776868814\n",
      "Epoch 324: train loss = 0.5011798990890385; eval loss = 0.8398474777738246\n",
      "Epoch 325: train loss = 0.4977533505298197; eval loss = 0.7006794530898336\n",
      "Epoch 326: train loss = 0.521403838104258; eval loss = 0.8555845171213154\n",
      "Epoch 327: train loss = 0.507801119548579; eval loss = 0.712871612980962\n",
      "Epoch 328: train loss = 0.4900731144783396; eval loss = 0.7207713332027194\n",
      "Epoch 329: train loss = 0.49074401613324886; eval loss = 0.7310277937601012\n",
      "Epoch 330: train loss = 0.48212689440697437; eval loss = 0.7899748515337705\n",
      "Epoch 331: train loss = 0.4798230403102935; eval loss = 0.8113775793462992\n",
      "Epoch 332: train loss = 0.5034310983804365; eval loss = 0.7716687607268485\n",
      "Epoch 333: train loss = 0.48217809293419134; eval loss = 0.9395504593849181\n",
      "Epoch 334: train loss = 0.5044445643822354; eval loss = 0.7560057006776328\n",
      "Epoch 335: train loss = 0.5120472301108142; eval loss = 0.7265628986060617\n",
      "Epoch 336: train loss = 0.5005684889232118; eval loss = 0.67354722879827\n",
      "Epoch 337: train loss = 0.5037237727083266; eval loss = 0.7060284893959755\n",
      "Epoch 338: train loss = 0.49704714197044575; eval loss = 0.7440169726808863\n",
      "Epoch 339: train loss = 0.5153437621581057; eval loss = 0.7464849073439837\n",
      "Epoch 340: train loss = 0.4785412640000383; eval loss = 0.7900767233222727\n",
      "Epoch 341: train loss = 0.4984644859408341; eval loss = 1.1478841230273238\n",
      "Epoch 342: train loss = 0.5453168936073779; eval loss = 0.8060808945447209\n",
      "Epoch 343: train loss = 0.5190902381824951; eval loss = 1.1401948202401397\n",
      "Epoch 344: train loss = 0.53538640604044; eval loss = 0.8252815809100871\n",
      "Epoch 345: train loss = 0.5033332079959413; eval loss = 0.8291511163115506\n",
      "Epoch 346: train loss = 0.5000685358730456; eval loss = 0.7754542337109641\n",
      "Epoch 347: train loss = 0.483078795329978; eval loss = 0.9057877237598103\n",
      "Epoch 348: train loss = 0.5119408879739543; eval loss = 0.7392080798745154\n",
      "Epoch 349: train loss = 0.4769066929196318; eval loss = 0.7593587990850212\n",
      "Epoch 350: train loss = 0.4703204836696385; eval loss = 0.7936166860163207\n",
      "Epoch 351: train loss = 0.5174973408381144; eval loss = 0.8041747827082869\n",
      "Epoch 352: train loss = 0.5165407800426086; eval loss = 0.8495080918073655\n",
      "Epoch 353: train loss = 0.5267959400080143; eval loss = 0.8726970050483944\n",
      "Epoch 354: train loss = 0.5178865613415836; eval loss = 0.8339283385624491\n",
      "Epoch 355: train loss = 0.5056185902406772; eval loss = 0.807111764326692\n",
      "Epoch 356: train loss = 0.4850526872711877; eval loss = 0.7756910038491087\n",
      "Epoch 357: train loss = 0.48582042971005046; eval loss = 0.766154060761134\n",
      "Epoch 358: train loss = 0.4766928151560326; eval loss = 0.9198727825035652\n",
      "Epoch 359: train loss = 0.49874587527786685; eval loss = 0.8200888919333617\n",
      "Epoch 360: train loss = 0.5112690255045891; eval loss = 0.7046046033501623\n",
      "Epoch 361: train loss = 0.4945175196044147; eval loss = 0.7368522166673623\n",
      "Epoch 362: train loss = 0.47739689610898495; eval loss = 0.761418271809817\n",
      "Epoch 363: train loss = 0.46869113234182197; eval loss = 0.8123059347271921\n",
      "Epoch 364: train loss = 0.5146388942375779; eval loss = 0.8230767603963618\n",
      "Epoch 365: train loss = 0.5166747584007682; eval loss = 1.2402166221290816\n",
      "Epoch 366: train loss = 0.541266839019954; eval loss = 0.8004565779119731\n",
      "Epoch 367: train loss = 0.5190889496977131; eval loss = 1.206626342609524\n",
      "Epoch 368: train loss = 0.5278096507924299; eval loss = 0.7873432746245747\n",
      "Epoch 369: train loss = 0.5077591238853831; eval loss = 1.1167503384252386\n",
      "Epoch 370: train loss = 0.5023760331484177; eval loss = 0.7718390859663483\n",
      "Epoch 371: train loss = 0.5472963107749822; eval loss = 0.8109939976462293\n",
      "Epoch 372: train loss = 0.49606068494419264; eval loss = 0.7975905966013671\n",
      "Epoch 373: train loss = 0.5074547122543056; eval loss = 0.7818212751299138\n",
      "Epoch 374: train loss = 0.4851827103023728; eval loss = 0.7869369257241484\n",
      "Epoch 375: train loss = 0.5013566163058083; eval loss = 0.8506976967411379\n",
      "Epoch 376: train loss = 0.5008386767779787; eval loss = 0.824159402400255\n",
      "Epoch 377: train loss = 0.5016907279690106; eval loss = 0.7617294397205112\n",
      "Epoch 378: train loss = 0.47917149433245265; eval loss = 1.0750302914530032\n",
      "Epoch 379: train loss = 0.5217662841702502; eval loss = 1.0281282737851143\n",
      "Epoch 380: train loss = 0.5182411223649979; eval loss = 0.9462138526141645\n",
      "Epoch 381: train loss = 0.5005618038897715; eval loss = 0.9151657745242123\n",
      "Epoch 382: train loss = 0.493537415129443; eval loss = 0.8628112878650422\n",
      "Epoch 383: train loss = 0.507310169438521; eval loss = 0.7560530068973699\n",
      "Epoch 384: train loss = 0.5100781417762239; eval loss = 0.7445171659014054\n",
      "Epoch 385: train loss = 0.5052301697432995; eval loss = 0.7396822031587359\n",
      "Epoch 386: train loss = 0.5037915968957047; eval loss = 0.7950277552008629\n",
      "Epoch 387: train loss = 0.5060429836933813; eval loss = 0.7837562654167415\n",
      "Epoch 388: train loss = 0.5041060289368035; eval loss = 0.7536032851785418\n",
      "Epoch 389: train loss = 0.5156962132702271; eval loss = 0.7552705543736622\n",
      "Epoch 390: train loss = 0.5151911174568038; eval loss = 0.7562218904495241\n",
      "Epoch 391: train loss = 0.5194165116796892; eval loss = 1.1237019741286833\n",
      "Epoch 392: train loss = 0.529086170718074; eval loss = 0.8058394479254886\n",
      "Epoch 393: train loss = 0.5228608033309381; eval loss = 0.8153389431536197\n",
      "Epoch 394: train loss = 0.5246447573105494; eval loss = 0.9092062152922157\n",
      "Epoch 395: train loss = 0.5218004464792709; eval loss = 0.8910701069980858\n",
      "Epoch 396: train loss = 0.5297672374484442; eval loss = 0.8236788790673018\n",
      "Epoch 397: train loss = 0.5279233258528013; eval loss = 0.837421132251621\n",
      "Epoch 398: train loss = 0.5430451530652741; eval loss = 0.8024212467883313\n",
      "Epoch 399: train loss = 0.5421427770828209; eval loss = 0.8264466654509299\n",
      "Epoch 400: train loss = 0.7509757828277848; eval loss = 1.1378200960656013\n",
      "Epoch 401: train loss = 0.7360055204480885; eval loss = 1.5108331106603141\n",
      "Epoch 402: train loss = 0.7874930668622255; eval loss = 1.5309919379651555\n",
      "Epoch 403: train loss = 0.784267041211327; eval loss = 1.458918392658234\n",
      "Epoch 404: train loss = 0.7648622514680027; eval loss = 1.2550340257585046\n",
      "Epoch 405: train loss = 0.7935357820242642; eval loss = 1.617612790316343\n",
      "Epoch 406: train loss = 0.8004365125671028; eval loss = 1.5361649505794055\n",
      "Epoch 407: train loss = 0.7509599855790535; eval loss = 1.2917958647012717\n",
      "Epoch 408: train loss = 0.8028073599562047; eval loss = 1.2722911983728407\n",
      "Epoch 409: train loss = 0.8043983637665706; eval loss = 1.2923279888927937\n",
      "Epoch 410: train loss = 0.803714017383754; eval loss = 1.908935160686572\n",
      "Epoch 411: train loss = 0.8076115278527141; eval loss = 1.3322370667010552\n",
      "Epoch 412: train loss = 0.8246507275228702; eval loss = 1.5119974526266264\n",
      "Epoch 413: train loss = 0.7779222568497061; eval loss = 1.496171101927757\n",
      "Epoch 414: train loss = 0.794641162889699; eval loss = 1.5477395995209615\n",
      "Epoch 415: train loss = 0.7966589281956351; eval loss = 1.5582524128258228\n",
      "Epoch 416: train loss = 0.7893416682879132; eval loss = 1.5571427717804902\n",
      "Epoch 417: train loss = 0.7878802185878159; eval loss = 1.4798084385693073\n",
      "Epoch 418: train loss = 0.8121224536250037; eval loss = 1.5990080808599798\n",
      "Epoch 419: train loss = 0.8151634270325303; eval loss = 1.5606213969488933\n",
      "Epoch 420: train loss = 0.8251870609819887; eval loss = 1.4557168819010267\n",
      "Epoch 421: train loss = 0.7977030814314879; eval loss = 1.5591345963378733\n",
      "Epoch 422: train loss = 0.8328454596921799; eval loss = 1.4396614581346514\n",
      "Epoch 423: train loss = 0.8505062246695158; eval loss = 1.5542197786271565\n",
      "Epoch 424: train loss = 0.864145686849952; eval loss = 1.4760854598134758\n",
      "Epoch 425: train loss = 0.8547800295054913; eval loss = 1.4072296582162382\n",
      "Epoch 426: train loss = 0.8469795323908329; eval loss = 1.412512108683585\n",
      "Epoch 427: train loss = 0.8758159714440507; eval loss = 1.4210369847714912\n",
      "Epoch 428: train loss = 0.8729822579771278; eval loss = 1.4162634468326973\n",
      "Epoch 429: train loss = 0.8825705020378033; eval loss = 1.4342852632204686\n",
      "Epoch 430: train loss = 0.8868608226378759; eval loss = 2.0840464296440286\n",
      "Epoch 431: train loss = 0.8515086406841874; eval loss = 2.0388746627916903\n",
      "Epoch 432: train loss = 0.8611420886591076; eval loss = 1.5131534087870806\n",
      "Epoch 433: train loss = 0.938109440729022; eval loss = 1.6274326927959908\n",
      "Epoch 434: train loss = 0.9259797235329946; eval loss = 1.5972201128800712\n",
      "Epoch 435: train loss = 0.9009957055871686; eval loss = 1.616388297329347\n",
      "Epoch 436: train loss = 0.8672991525381804; eval loss = 1.5074717923998833\n",
      "Epoch 437: train loss = 0.8857973900934059; eval loss = 1.4866403788328166\n",
      "Epoch 438: train loss = 0.9004667941480874; eval loss = 1.5994077523549388\n",
      "Epoch 439: train loss = 0.8639388683562479; eval loss = 1.551242832715313\n",
      "Epoch 440: train loss = 0.8738978874559206; eval loss = 1.6034260205924515\n",
      "Epoch 441: train loss = 0.8550190503398577; eval loss = 1.6437956616282454\n",
      "Epoch 442: train loss = 0.8628378252809247; eval loss = 1.6103784057001271\n",
      "Epoch 443: train loss = 0.8354925677801172; eval loss = 1.6037377938628194\n",
      "Epoch 444: train loss = 0.8542524349565307; eval loss = 1.504604522138834\n",
      "Epoch 445: train loss = 0.8456593599791332; eval loss = 1.4568676017224786\n",
      "Epoch 446: train loss = 0.8342979261651635; eval loss = 1.4963050447404378\n",
      "Epoch 447: train loss = 0.8192778571198382; eval loss = 1.6486360952258112\n",
      "Epoch 448: train loss = 0.8566928307215371; eval loss = 1.5613543850680207\n",
      "Epoch 449: train loss = 0.8437559263159832; eval loss = 1.5518576775987936\n",
      "Epoch 450: train loss = 0.844187811948359; eval loss = 1.4913047440350065\n",
      "Epoch 451: train loss = 0.8459678621341785; eval loss = 1.476785299678643\n",
      "Epoch 452: train loss = 0.9138588874290388; eval loss = 1.5443196073174483\n",
      "Epoch 453: train loss = 0.9039055764054262; eval loss = 1.4290573187172422\n",
      "Epoch 454: train loss = 0.915244534922143; eval loss = 1.3879180066287524\n",
      "Epoch 455: train loss = 0.9097309596836568; eval loss = 1.3622468411922444\n",
      "Epoch 456: train loss = 0.9034364729498824; eval loss = 1.3533229678869243\n",
      "Epoch 457: train loss = 0.8903715095172328; eval loss = 1.5673148036003108\n",
      "Epoch 458: train loss = 0.8677293835207822; eval loss = 1.51384948939085\n",
      "Epoch 459: train loss = 0.8614499407509961; eval loss = 1.5145157271375262\n",
      "Epoch 460: train loss = 0.854190303944051; eval loss = 1.507518332451582\n",
      "Epoch 461: train loss = 0.8562606734534104; eval loss = 1.5022406503558159\n",
      "Epoch 462: train loss = 0.8698839169616499; eval loss = 1.8213990032672873\n",
      "Epoch 463: train loss = 0.9316122801974416; eval loss = 1.4580657215168087\n",
      "Epoch 464: train loss = 0.9447279541442792; eval loss = 1.4266887940466408\n",
      "Epoch 465: train loss = 0.9532079063355923; eval loss = 1.508204467594623\n",
      "Epoch 466: train loss = 1.0075043731679518; eval loss = 1.4848157277419443\n",
      "Epoch 467: train loss = 0.9203501759717863; eval loss = 1.4854114279150954\n",
      "Epoch 468: train loss = 0.9123013764619831; eval loss = 1.747085116803645\n",
      "Epoch 469: train loss = 0.9481738430137434; eval loss = 1.6283180117607114\n",
      "Epoch 470: train loss = 1.0208205934613943; eval loss = 1.568331855038802\n",
      "Epoch 471: train loss = 0.9425821540256344; eval loss = 1.7561015722652278\n",
      "Epoch 472: train loss = 0.9819745648031436; eval loss = 1.5854552127420896\n",
      "Epoch 473: train loss = 1.0294553513328233; eval loss = 1.5819019048164285\n",
      "Epoch 474: train loss = 0.9803948185096183; eval loss = 1.582905389368535\n",
      "Epoch 475: train loss = 0.9970832299441101; eval loss = 1.3882906372171073\n",
      "Epoch 476: train loss = 0.9613920242215198; eval loss = 1.482276055961848\n",
      "Epoch 477: train loss = 0.9788671409090359; eval loss = 1.5275716707110394\n",
      "Epoch 478: train loss = 1.0154156271989148; eval loss = 1.3949209426840132\n",
      "Epoch 479: train loss = 1.000554493938883; eval loss = 1.336032327264548\n",
      "Epoch 480: train loss = 0.9917154883344966; eval loss = 1.3210961328198512\n",
      "Epoch 481: train loss = 0.9836638666068515; eval loss = 2.2305087186396126\n",
      "Epoch 482: train loss = 1.0087204227844875; eval loss = 2.005527961999177\n",
      "Epoch 483: train loss = 1.0033387321357923; eval loss = 1.3434110234181085\n",
      "Epoch 484: train loss = 0.9861685677121084; eval loss = 1.7654388323426258\n",
      "Epoch 485: train loss = 0.9179160126174488; eval loss = 1.7001584693789484\n",
      "Epoch 486: train loss = 0.9153791237622497; eval loss = 1.6494167670607556\n",
      "Epoch 487: train loss = 0.9245490950221819; eval loss = 1.6303834244608872\n",
      "Epoch 488: train loss = 0.9464582086851203; eval loss = 1.3308026641607293\n",
      "Epoch 489: train loss = 0.9710046223675209; eval loss = 1.3032300384449116\n",
      "Epoch 490: train loss = 0.9662540197993319; eval loss = 2.1691243797540674\n",
      "Epoch 491: train loss = 0.9754910680154961; eval loss = 1.3147630952298646\n",
      "Epoch 492: train loss = 0.9668927434831858; eval loss = 1.309951467439531\n",
      "Epoch 493: train loss = 0.9573393060515323; eval loss = 2.224144864827393\n",
      "Epoch 494: train loss = 0.956090992006163; eval loss = 1.3082015482442721\n",
      "Epoch 495: train loss = 0.9395397535214819; eval loss = 2.2029090188443674\n",
      "Epoch 496: train loss = 0.9501842372119426; eval loss = 1.3634556308388717\n",
      "Epoch 497: train loss = 0.945818724731604; eval loss = 2.310322917997838\n",
      "Epoch 498: train loss = 0.9564292430877682; eval loss = 1.4194877445697784\n",
      "Epoch 499: train loss = 0.9165739774083097; eval loss = 2.4045058339834213\n",
      "Epoch 500: train loss = 1.1523192621146636; eval loss = 1.4951359964907163\n",
      "Epoch 501: train loss = 1.0478668591628475; eval loss = 1.5524840982009966\n",
      "Epoch 502: train loss = 1.0175366597250104; eval loss = 1.5756249949336054\n",
      "Epoch 503: train loss = 1.0071201867734394; eval loss = 1.516583237797022\n",
      "Epoch 504: train loss = 1.0011876554538808; eval loss = 1.514099250237147\n",
      "Epoch 505: train loss = 0.9999863430857656; eval loss = 1.537942799429098\n",
      "Epoch 506: train loss = 0.9704725068683426; eval loss = 1.6037714257836342\n",
      "Epoch 507: train loss = 1.020980527003606; eval loss = 1.6369862693051498\n",
      "Epoch 508: train loss = 1.0152568202465773; eval loss = 1.4475083760917191\n",
      "Epoch 509: train loss = 0.993696593058606; eval loss = 1.4650414325296888\n",
      "Epoch 510: train loss = 0.9883945751935242; eval loss = 1.4536808058619504\n",
      "Epoch 511: train loss = 0.9857087753092248; eval loss = 1.447486277669668\n",
      "Epoch 512: train loss = 0.9883782189960284; eval loss = 1.411201256016891\n",
      "Epoch 513: train loss = 1.0565031518538792; eval loss = 1.3779243677854538\n",
      "Epoch 514: train loss = 1.0121752275153992; eval loss = 1.3704235007365555\n",
      "Epoch 515: train loss = 1.0001025789727769; eval loss = 1.4527132101356979\n",
      "Epoch 516: train loss = 1.007326263003051; eval loss = 1.4119335922102134\n",
      "Epoch 517: train loss = 1.0027852837617197; eval loss = 1.404043573886157\n",
      "Epoch 518: train loss = 0.9958718738829095; eval loss = 1.402529453858734\n",
      "Epoch 519: train loss = 0.9905065071458617; eval loss = 1.3995795771479598\n",
      "Epoch 520: train loss = 0.9860229489083089; eval loss = 2.084667786955833\n",
      "Epoch 521: train loss = 1.0200399806102118; eval loss = 1.4860638417303562\n",
      "Epoch 522: train loss = 1.0534529152015848; eval loss = 1.4248797409236427\n",
      "Epoch 523: train loss = 1.041805813709895; eval loss = 1.403575919568539\n",
      "Epoch 524: train loss = 1.035183370423814; eval loss = 1.3744254261255258\n",
      "Epoch 525: train loss = 1.027577317940692; eval loss = 1.354448402300477\n",
      "Epoch 526: train loss = 1.0210569441939392; eval loss = 1.3560642643521228\n",
      "Epoch 527: train loss = 1.0224832684422531; eval loss = 1.3280613178475973\n",
      "Epoch 528: train loss = 1.0168867421646912; eval loss = 1.3346521388739347\n",
      "Epoch 529: train loss = 1.0199321862310173; eval loss = 1.3139924947172403\n",
      "Epoch 530: train loss = 1.0119183690597615; eval loss = 1.2961037289351216\n",
      "Epoch 531: train loss = 1.0090026119723918; eval loss = 1.2812609300017361\n",
      "Epoch 532: train loss = 1.0109007054318981; eval loss = 2.016592673957348\n",
      "Epoch 533: train loss = 1.0165417998408277; eval loss = 1.3071077652275565\n",
      "Epoch 534: train loss = 1.0151530653238297; eval loss = 2.0149908885359755\n",
      "Epoch 535: train loss = 1.0216597833981116; eval loss = 1.8946443870663627\n",
      "Epoch 536: train loss = 1.028080329298973; eval loss = 1.41047753766179\n",
      "Epoch 537: train loss = 0.9997468708703914; eval loss = 2.0144403465092178\n",
      "Epoch 538: train loss = 1.0091267467165985; eval loss = 2.0780292823910713\n",
      "Epoch 539: train loss = 1.0182038157557447; eval loss = 1.9990053698420522\n",
      "Epoch 540: train loss = 1.0194362979382277; eval loss = 1.5348201955979066\n",
      "Epoch 541: train loss = 1.0204860537002485; eval loss = 2.0671611316502103\n",
      "Epoch 542: train loss = 1.0146065258110564; eval loss = 1.5261041447520252\n",
      "Epoch 543: train loss = 1.0357800377532838; eval loss = 2.0966080464422703\n",
      "Epoch 544: train loss = 1.0180895098795495; eval loss = 1.6912484802305705\n",
      "Epoch 545: train loss = 1.0130617159108322; eval loss = 1.6819943711161611\n",
      "Epoch 546: train loss = 0.9933971871311466; eval loss = 1.6388044444223244\n",
      "Epoch 547: train loss = 0.9831679152945678; eval loss = 1.626575071364641\n",
      "Epoch 548: train loss = 0.9785842240477602; eval loss = 1.6211641319096097\n",
      "Epoch 549: train loss = 0.9733659867197274; eval loss = 1.6015814766287804\n",
      "Epoch 550: train loss = 0.9671733900904653; eval loss = 1.5747485458850852\n",
      "Epoch 551: train loss = 0.9676500863085189; eval loss = 1.5662446555991967\n",
      "Epoch 552: train loss = 0.9701534981528918; eval loss = 1.5553133785724635\n",
      "Epoch 553: train loss = 0.9713038094341756; eval loss = 1.5238813236355786\n",
      "Epoch 554: train loss = 1.0089088628689447; eval loss = 1.5752006433904175\n",
      "Epoch 555: train loss = 1.025633196036021; eval loss = 1.4923204630613338\n",
      "Epoch 556: train loss = 1.002182953370114; eval loss = 1.4696883037686346\n",
      "Epoch 557: train loss = 1.0120591648543875; eval loss = 1.4797438172002633\n",
      "Epoch 558: train loss = 1.0071806159491339; eval loss = 1.6347357568641492\n",
      "Epoch 559: train loss = 0.9771580873057247; eval loss = 1.6193046458065512\n",
      "Epoch 560: train loss = 1.0091444856176774; eval loss = 1.5022635310888286\n",
      "Epoch 561: train loss = 1.0237477626651525; eval loss = 1.4956720694899557\n",
      "Epoch 562: train loss = 0.9770864102368552; eval loss = 1.5226719230413437\n",
      "Epoch 563: train loss = 0.9714015719170369; eval loss = 1.534131348133087\n",
      "Epoch 564: train loss = 0.9787827618420123; eval loss = 1.5429408252239227\n",
      "Epoch 565: train loss = 0.9733069151019059; eval loss = 1.4973246157169342\n",
      "Epoch 566: train loss = 0.9844024678071338; eval loss = 1.516101890908821\n",
      "Epoch 567: train loss = 1.0283531217525401; eval loss = 1.4767892624561978\n",
      "Epoch 568: train loss = 1.0540130473673346; eval loss = 1.4422115356262253\n",
      "Epoch 569: train loss = 1.0364768709987402; eval loss = 1.4541709492249144\n",
      "Epoch 570: train loss = 1.0710373396674793; eval loss = 1.4819852560758586\n",
      "Epoch 571: train loss = 1.0668977306534846; eval loss = 1.4674152880907052\n",
      "Epoch 572: train loss = 1.0573778121421737; eval loss = 1.430466599762439\n",
      "Epoch 573: train loss = 1.0379606128359833; eval loss = 1.396469614778956\n",
      "Epoch 574: train loss = 1.045159419377645; eval loss = 1.3892623074352737\n",
      "Epoch 575: train loss = 1.0443242747957506; eval loss = 1.3588379947468647\n",
      "Epoch 576: train loss = 1.0625529621417327; eval loss = 1.4799605794250967\n",
      "Epoch 577: train loss = 1.0592884055028358; eval loss = 1.4861476160585885\n",
      "Epoch 578: train loss = 1.0284523321315646; eval loss = 1.4564018957316873\n",
      "Epoch 579: train loss = 1.0150597151368856; eval loss = 1.5405452549457548\n",
      "Epoch 580: train loss = 1.0409727341805899; eval loss = 1.4455279322961962\n",
      "Epoch 581: train loss = 1.0090004277105133; eval loss = 1.4927645325660701\n",
      "Epoch 582: train loss = 1.0410059581821165; eval loss = 1.4882903918623918\n",
      "Epoch 583: train loss = 1.1225751520444947; eval loss = 1.7754251658916487\n",
      "Epoch 584: train loss = 1.1109687720114985; eval loss = 1.4291795467336978\n",
      "Epoch 585: train loss = 1.14583759692808; eval loss = 1.4233678467571727\n",
      "Epoch 586: train loss = 1.2076652658482392; eval loss = 1.4008207283914085\n",
      "Epoch 587: train loss = 1.1685396069660783; eval loss = 1.4702904628855842\n",
      "Epoch 588: train loss = 1.2405476104468105; eval loss = 1.6252479925751688\n",
      "Epoch 589: train loss = 1.2935685223589342; eval loss = 2.14131639773647\n",
      "Epoch 590: train loss = 1.2989346155275905; eval loss = 1.7127363905310637\n",
      "Epoch 591: train loss = 1.250665115813414; eval loss = 1.5430056080222132\n",
      "Epoch 592: train loss = 1.2965868022292852; eval loss = 1.56320559978485\n",
      "Epoch 593: train loss = 1.381312398860852; eval loss = 1.4887035116553295\n",
      "Epoch 594: train loss = 1.3612741554776824; eval loss = 1.406275589019059\n",
      "Epoch 595: train loss = 1.2989336904138329; eval loss = 1.3709291741251948\n",
      "Epoch 596: train loss = 1.2014143584916992; eval loss = 1.4814173597842444\n",
      "Epoch 597: train loss = 1.2743045041958492; eval loss = 1.3796808458864696\n",
      "Epoch 598: train loss = 1.278195743138591; eval loss = 1.950687443216641\n",
      "Epoch 599: train loss = 1.3605553697173796; eval loss = 1.3873012512922287\n",
      "Epoch 600: train loss = 1.3768838476389644; eval loss = 1.3708747774362564\n",
      "Epoch 601: train loss = 1.406035827472806; eval loss = 2.5264301449060422\n",
      "Epoch 602: train loss = 1.6089650491873422; eval loss = 1.4371732547879221\n",
      "Epoch 603: train loss = 1.4696466624736788; eval loss = 1.7812022678554067\n",
      "Epoch 604: train loss = 1.4556814314176634; eval loss = 1.358288590485851\n",
      "Epoch 605: train loss = 1.4719301654646793; eval loss = 1.3824885301291938\n",
      "Epoch 606: train loss = 1.4994504066805046; eval loss = 1.5087737068533902\n",
      "Epoch 607: train loss = 1.5415615166227021; eval loss = 1.6254355758428574\n",
      "Epoch 608: train loss = 1.6133630486826105; eval loss = 1.631671063601972\n",
      "Epoch 609: train loss = 1.6327276999751725; eval loss = 1.5935904383659374\n",
      "Epoch 610: train loss = 1.5390656075129912; eval loss = 2.4529213234782223\n",
      "Epoch 611: train loss = 1.4112810511142015; eval loss = 1.4896223880350588\n",
      "Epoch 612: train loss = 1.3819629941135647; eval loss = 1.4793108031153683\n",
      "Epoch 613: train loss = 1.402923854378362; eval loss = 1.4791980907320976\n",
      "Epoch 614: train loss = 1.4045229995002344; eval loss = 1.4647755958139903\n",
      "Epoch 615: train loss = 1.400283383515974; eval loss = 1.4462088644504545\n",
      "Epoch 616: train loss = 1.3880258289476237; eval loss = 1.4179957918822774\n",
      "Epoch 617: train loss = 1.373523152122895; eval loss = 1.512341953814029\n",
      "Epoch 618: train loss = 1.3443833837906523; eval loss = 1.5273727141320699\n",
      "Epoch 619: train loss = 1.307788509254654; eval loss = 1.494294742743174\n",
      "Epoch 620: train loss = 1.2523478691776597; eval loss = 1.4249719319244227\n",
      "Epoch 621: train loss = 1.2343446773787339; eval loss = 1.4346492923796166\n",
      "Epoch 622: train loss = 1.2165673213700456; eval loss = 1.4123752638697635\n",
      "Epoch 623: train loss = 1.19988476857543; eval loss = 1.4040345624089239\n",
      "Epoch 624: train loss = 1.1856308231751123; eval loss = 1.4119846262037754\n",
      "Epoch 625: train loss = 1.1755738730231917; eval loss = 1.3950221538543697\n",
      "Epoch 626: train loss = 1.1665398341914017; eval loss = 1.3795419484376898\n",
      "Epoch 627: train loss = 1.159098977223039; eval loss = 1.3657990445693329\n",
      "Epoch 628: train loss = 1.1514450162649155; eval loss = 1.3507821857929232\n",
      "Epoch 629: train loss = 1.142544386908412; eval loss = 1.3455610498785984\n",
      "Epoch 630: train loss = 1.1364494841545822; eval loss = 1.345325784136852\n",
      "Epoch 631: train loss = 1.1361062582582238; eval loss = 1.3085972145199785\n",
      "Epoch 632: train loss = 1.1315822483350837; eval loss = 1.2458279281854627\n",
      "Epoch 633: train loss = 1.126709800834457; eval loss = 1.2351939206322031\n",
      "Epoch 634: train loss = 1.1057044373204312; eval loss = 1.1153910644352443\n",
      "Epoch 635: train loss = 1.0876032986367745; eval loss = 1.1265730969607834\n",
      "Epoch 636: train loss = 1.06221785955131; eval loss = 1.209358591586351\n",
      "Epoch 637: train loss = 1.0729252584278584; eval loss = 1.1537034809589382\n",
      "Epoch 638: train loss = 1.0829343839238088; eval loss = 1.146764078487953\n",
      "Epoch 639: train loss = 1.083344311142961; eval loss = 1.1158547401428218\n",
      "Epoch 640: train loss = 1.0993567646170659; eval loss = 1.1202527917921539\n",
      "Epoch 641: train loss = 1.0963898642609515; eval loss = 1.1978623606264587\n",
      "Epoch 642: train loss = 1.0502936644479635; eval loss = 1.1560058146715164\n",
      "Epoch 643: train loss = 1.0311496791740258; eval loss = 1.189056534320116\n",
      "Epoch 644: train loss = 1.0208688067893188; eval loss = 1.212707333266735\n",
      "Epoch 645: train loss = 1.0179315013810992; eval loss = 1.2103915959596627\n",
      "Epoch 646: train loss = 1.025379354134202; eval loss = 1.2198932468891144\n",
      "Epoch 647: train loss = 1.0136706642806532; eval loss = 1.2355512604117398\n",
      "Epoch 648: train loss = 1.0110985779513915; eval loss = 1.2535532936453824\n",
      "Epoch 649: train loss = 1.0122026739021142; eval loss = 1.2809250280261046\n",
      "Epoch 650: train loss = 1.0092794786517816; eval loss = 1.2766948863863943\n",
      "Epoch 651: train loss = 1.0084589595595999; eval loss = 1.2642877101898193\n",
      "Epoch 652: train loss = 1.009168116996686; eval loss = 1.262492060661316\n",
      "Epoch 653: train loss = 1.0099356401090822; eval loss = 1.2512821406126022\n",
      "Epoch 654: train loss = 1.0124504510313272; eval loss = 1.2636120170354834\n",
      "Epoch 655: train loss = 1.01177222126474; eval loss = 1.251464322209359\n",
      "Epoch 656: train loss = 1.0096335566292205; eval loss = 1.269562780857086\n",
      "Epoch 657: train loss = 1.0059171399722497; eval loss = 1.2990501299500463\n",
      "Epoch 658: train loss = 0.999423923591773; eval loss = 1.426857151091099\n",
      "Epoch 659: train loss = 1.112997791108986; eval loss = 1.5474492423236377\n",
      "Epoch 660: train loss = 1.0554608982056382; eval loss = 1.3925526291131969\n",
      "Epoch 661: train loss = 0.9979276287679871; eval loss = 1.3828177303075788\n",
      "Epoch 662: train loss = 0.9899200151364007; eval loss = 1.382897034287452\n",
      "Epoch 663: train loss = 0.9974231598898768; eval loss = 1.3802757933735856\n",
      "Epoch 664: train loss = 0.9866425724079214; eval loss = 1.1700499653816223\n",
      "Epoch 665: train loss = 1.0085255118707817; eval loss = 1.3097160495817666\n",
      "Epoch 666: train loss = 0.9932619240134956; eval loss = 1.3155685005088649\n",
      "Epoch 667: train loss = 0.9992137781033914; eval loss = 1.3209487870335574\n",
      "Epoch 668: train loss = 1.00249067073067; eval loss = 1.3368390277028086\n",
      "Epoch 669: train loss = 1.0102812182158232; eval loss = 1.3558434322476391\n",
      "Epoch 670: train loss = 1.018914540608724; eval loss = 1.409783653914928\n",
      "Epoch 671: train loss = 1.0881963626792035; eval loss = 1.5017735362052915\n",
      "Epoch 672: train loss = 1.0226032864302397; eval loss = 1.796543095260859\n",
      "Epoch 673: train loss = 1.0692151455829546; eval loss = 1.1543572116643195\n",
      "Epoch 674: train loss = 1.0540468248849113; eval loss = 1.1431542802602048\n",
      "Epoch 675: train loss = 1.0572649252911408; eval loss = 1.9196978434920315\n",
      "Epoch 676: train loss = 1.0689766295254228; eval loss = 1.922906026244163\n",
      "Epoch 677: train loss = 1.0620665829628708; eval loss = 1.2478595674037927\n",
      "Epoch 678: train loss = 1.0467500211670995; eval loss = 1.2370011620223527\n",
      "Epoch 679: train loss = 1.0426767170429228; eval loss = 1.2413704842329025\n",
      "Epoch 680: train loss = 1.0449930302177868; eval loss = 1.2274479083716867\n",
      "Epoch 681: train loss = 1.0384799173722665; eval loss = 1.223038490861655\n",
      "Epoch 682: train loss = 1.035410958652695; eval loss = 1.3793275021016593\n",
      "Epoch 683: train loss = 0.9944326359157759; eval loss = 1.352461762726306\n",
      "Epoch 684: train loss = 0.9860894959419967; eval loss = 1.4748367816209793\n",
      "Epoch 685: train loss = 0.9948727165659269; eval loss = 1.4201871715486039\n",
      "Epoch 686: train loss = 0.9977971417829397; eval loss = 1.3822707682847963\n",
      "Epoch 687: train loss = 0.9927315187330046; eval loss = 1.3623066879808896\n",
      "Epoch 688: train loss = 0.9917150158435106; eval loss = 1.363164495676756\n",
      "Epoch 689: train loss = 0.9946824663008255; eval loss = 1.3566043227910998\n",
      "Epoch 690: train loss = 0.9963604031751552; eval loss = 1.36178158968687\n",
      "Epoch 691: train loss = 0.9987151734530926; eval loss = 1.3618683069944393\n",
      "Epoch 692: train loss = 0.9994622751449547; eval loss = 1.3626715764403345\n",
      "Epoch 693: train loss = 0.9907940883810323; eval loss = 1.2759440205991268\n",
      "Epoch 694: train loss = 0.9995437376201152; eval loss = 1.2797396667301648\n",
      "Epoch 695: train loss = 1.007178186128537; eval loss = 1.365242622792721\n",
      "Epoch 696: train loss = 0.994927811436355; eval loss = 1.6072401106357572\n",
      "Epoch 697: train loss = 1.0574027942493558; eval loss = 1.8652075007557878\n",
      "Epoch 698: train loss = 1.0483949944997826; eval loss = 1.2727315090596676\n",
      "Epoch 699: train loss = 1.0585082434117794; eval loss = 1.3227614723145953\n",
      "Epoch 700: train loss = 1.0348720919961731; eval loss = 1.3214200139045724\n",
      "Epoch 701: train loss = 1.0327500753725567; eval loss = 1.3229743354022503\n",
      "Epoch 702: train loss = 1.0319639242564638; eval loss = 1.3239718563854685\n",
      "Epoch 703: train loss = 1.0316323523099222; eval loss = 1.3250818513333793\n",
      "Epoch 704: train loss = 1.0307337880755465; eval loss = 1.3266981542110452\n",
      "Epoch 705: train loss = 1.0296526302893956; eval loss = 1.32673791795969\n",
      "Epoch 706: train loss = 1.0290650976821782; eval loss = 1.3286287598311897\n",
      "Epoch 707: train loss = 1.0281007410958412; eval loss = 1.3330559916794305\n",
      "Epoch 708: train loss = 1.0223220111802218; eval loss = 1.3328542932868004\n",
      "Epoch 709: train loss = 1.0219579956804714; eval loss = 1.3295084685087195\n",
      "Epoch 710: train loss = 1.0212133135646582; eval loss = 1.3370521292090425\n",
      "Epoch 711: train loss = 1.0142189910014472; eval loss = 1.3269301988184448\n",
      "Epoch 712: train loss = 1.012785048224032; eval loss = 1.3295175433158881\n",
      "Epoch 713: train loss = 1.0107304692889254; eval loss = 1.32653021812439\n",
      "Epoch 714: train loss = 1.00376526111116; eval loss = 1.316246077418328\n",
      "Epoch 715: train loss = 1.0086603763823712; eval loss = 1.3340833075344563\n",
      "Epoch 716: train loss = 1.0143913061668477; eval loss = 1.3769096471369267\n",
      "Epoch 717: train loss = 1.0007713316008449; eval loss = 1.2836423031985764\n",
      "Epoch 718: train loss = 1.0428145121162138; eval loss = 1.81356953829527\n",
      "Epoch 719: train loss = 1.0186795635769765; eval loss = 1.8449191898107538\n",
      "Epoch 720: train loss = 1.013405998237431; eval loss = 1.877385273575783\n",
      "Epoch 721: train loss = 1.0065281822656589; eval loss = 1.340124271810054\n",
      "Epoch 722: train loss = 1.0120952374612293; eval loss = 1.3193006590008731\n",
      "Epoch 723: train loss = 0.9964113775640726; eval loss = 1.5636816993355755\n",
      "Epoch 724: train loss = 0.9884086614474655; eval loss = 1.4491114988923073\n",
      "Epoch 725: train loss = 0.9557126552487414; eval loss = 1.3829576149582867\n",
      "Epoch 726: train loss = 1.0187114505097266; eval loss = 1.3641594946384428\n",
      "Epoch 727: train loss = 0.9949508206918838; eval loss = 1.3580417633056634\n",
      "Epoch 728: train loss = 0.9967367652182779; eval loss = 1.481441050767898\n",
      "Epoch 729: train loss = 0.9341955451915657; eval loss = 1.5264302194118498\n",
      "Epoch 730: train loss = 0.9494781540706753; eval loss = 1.444336332380772\n",
      "Epoch 731: train loss = 0.9307730486616491; eval loss = 1.316155456006526\n",
      "Epoch 732: train loss = 0.9692130498588086; eval loss = 1.4134608395397663\n",
      "Epoch 733: train loss = 0.946913318708539; eval loss = 1.3929828815162189\n",
      "Epoch 734: train loss = 0.957675112100939; eval loss = 1.3427124619483948\n",
      "Epoch 735: train loss = 0.9887860029314957; eval loss = 1.3781209625303747\n",
      "Epoch 736: train loss = 0.967791523784399; eval loss = 1.3483984656631935\n",
      "Epoch 737: train loss = 0.9687427437553803; eval loss = 1.3017511293292061\n",
      "Epoch 738: train loss = 1.0103254712497192; eval loss = 1.3329809717833982\n",
      "Epoch 739: train loss = 0.9799591908231374; eval loss = 1.5851741284132006\n",
      "Epoch 740: train loss = 0.9973414217432338; eval loss = 1.3785096779465675\n",
      "Epoch 741: train loss = 0.9698501086483399; eval loss = 1.5004168041050445\n",
      "Epoch 742: train loss = 0.9573723835249743; eval loss = 1.4886627085506923\n",
      "Epoch 743: train loss = 0.9707134570926426; eval loss = 1.3932386673986916\n",
      "Epoch 744: train loss = 0.9488349696621298; eval loss = 1.4553754515945918\n",
      "Epoch 745: train loss = 0.9797979217643538; eval loss = 1.3072076067328446\n",
      "Epoch 746: train loss = 0.9833292346447707; eval loss = 1.2803968787193294\n",
      "Epoch 747: train loss = 0.9876371171946327; eval loss = 1.289921976625919\n",
      "Epoch 748: train loss = 1.0096951831753056; eval loss = 1.2890583351254454\n",
      "Epoch 749: train loss = 1.0274534191315374; eval loss = 1.3518333882093432\n",
      "Epoch 750: train loss = 0.9736232133582234; eval loss = 1.4019710086286061\n",
      "Epoch 751: train loss = 0.946215679558615; eval loss = 1.4237478971481314\n",
      "Epoch 752: train loss = 0.9649822522575661; eval loss = 1.4187918677926064\n",
      "Epoch 753: train loss = 0.9644433148205281; eval loss = 1.4205180704593663\n",
      "Epoch 754: train loss = 0.9637381279220184; eval loss = 1.4223536700010297\n",
      "Epoch 755: train loss = 0.9595753181104857; eval loss = 1.4367954321205612\n",
      "Epoch 756: train loss = 0.9617530154064295; eval loss = 1.4388744123280055\n",
      "Epoch 757: train loss = 0.9626860640322167; eval loss = 1.4387644492089744\n",
      "Epoch 758: train loss = 0.9631704663236936; eval loss = 1.4406734779477126\n",
      "Epoch 759: train loss = 0.9640608544771869; eval loss = 1.4343871772289272\n",
      "Epoch 760: train loss = 0.963041104376316; eval loss = 1.4420165866613388\n",
      "Epoch 761: train loss = 0.9611901162813105; eval loss = 1.4402962327003475\n",
      "Epoch 762: train loss = 0.9641979097699126; eval loss = 1.432799231261015\n",
      "Epoch 763: train loss = 0.9599196851874394; eval loss = 1.4343944489955902\n",
      "Epoch 764: train loss = 0.9576522745192051; eval loss = 1.4304076731204993\n",
      "Epoch 765: train loss = 0.9539418763791524; eval loss = 1.4202838689088817\n",
      "Epoch 766: train loss = 0.9529921940217415; eval loss = 1.4171472527086735\n",
      "Epoch 767: train loss = 0.9526490255569416; eval loss = 1.417717855423688\n",
      "Epoch 768: train loss = 0.9497932015607753; eval loss = 1.4227692149579525\n",
      "Epoch 769: train loss = 0.948108808758358; eval loss = 1.4181103333830833\n",
      "Epoch 770: train loss = 0.9450837677965561; eval loss = 1.421494118869304\n",
      "Epoch 771: train loss = 0.9447287404909727; eval loss = 1.4200977645814417\n",
      "Epoch 772: train loss = 0.9464141667510074; eval loss = 1.41834931448102\n",
      "Epoch 773: train loss = 0.9423138396814463; eval loss = 1.421778593212366\n",
      "Epoch 774: train loss = 0.9439162258058785; eval loss = 1.4225474223494532\n",
      "Epoch 775: train loss = 0.9456788524985313; eval loss = 1.4237039275467398\n",
      "Epoch 776: train loss = 0.9387544576699536; eval loss = 1.4263234138488776\n",
      "Epoch 777: train loss = 0.9372842200100421; eval loss = 1.4312280640006068\n",
      "Epoch 778: train loss = 0.9398946777607005; eval loss = 1.431919526308774\n",
      "Epoch 779: train loss = 0.9368993733078242; eval loss = 1.4439390674233439\n",
      "Epoch 780: train loss = 0.9316034757842621; eval loss = 1.4403475448489182\n",
      "Epoch 781: train loss = 0.9349462315440178; eval loss = 1.4321125671267496\n",
      "Epoch 782: train loss = 0.9349256893619895; eval loss = 1.4285538531839848\n",
      "Epoch 783: train loss = 0.9327713173503674; eval loss = 1.4270769618451593\n",
      "Epoch 784: train loss = 0.931593159524103; eval loss = 1.4261188507080067\n",
      "Epoch 785: train loss = 0.9314082612593969; eval loss = 1.4288826659321783\n",
      "Epoch 786: train loss = 0.9289043356354041; eval loss = 1.4227233417332175\n",
      "Epoch 787: train loss = 0.9362150651092331; eval loss = 1.4262592196464543\n",
      "Epoch 788: train loss = 0.95832611558338; eval loss = 1.3282049968838685\n",
      "Epoch 789: train loss = 0.9691716128339372; eval loss = 1.3172987550497055\n",
      "Epoch 790: train loss = 0.9720869731778901; eval loss = 1.3196867182850833\n",
      "Epoch 791: train loss = 0.9720217973614731; eval loss = 1.3332096636295325\n",
      "Epoch 792: train loss = 0.9732189495116474; eval loss = 1.8938157185912132\n",
      "Epoch 793: train loss = 0.9562012137224278; eval loss = 1.3725799918174733\n",
      "Epoch 794: train loss = 0.9772967845201492; eval loss = 1.935415837913752\n",
      "Epoch 795: train loss = 0.9629252270484965; eval loss = 1.5063021481037142\n",
      "Epoch 796: train loss = 0.9448215542361139; eval loss = 1.425059799104929\n",
      "Epoch 797: train loss = 0.940426648283998; eval loss = 1.382839106023311\n",
      "Epoch 798: train loss = 0.9980016549428302; eval loss = 1.3753816187381749\n",
      "Epoch 799: train loss = 0.9925139403591552; eval loss = 1.3641736507415774\n",
      "Epoch 800: train loss = 0.9866860521336399; eval loss = 1.3632722571492206\n",
      "Epoch 801: train loss = 0.9771123854443431; eval loss = 2.016154088079929\n",
      "Epoch 802: train loss = 0.9696410043785969; eval loss = 1.4811497405171392\n",
      "Epoch 803: train loss = 0.9292388390749695; eval loss = 1.5217892751097684\n",
      "Epoch 804: train loss = 0.9384983119865258; eval loss = 1.9007229097187517\n",
      "Epoch 805: train loss = 0.9712072852998973; eval loss = 1.443727612495423\n",
      "Epoch 806: train loss = 0.9703294678280753; eval loss = 1.9232740215957147\n",
      "Epoch 807: train loss = 0.9663367451479035; eval loss = 1.9496321752667427\n",
      "Epoch 808: train loss = 0.9713375276575484; eval loss = 1.9469095431268202\n",
      "Epoch 809: train loss = 0.9581640083342792; eval loss = 1.5666920915246012\n",
      "Epoch 810: train loss = 1.0070270306120317; eval loss = 1.539691161364316\n",
      "Epoch 811: train loss = 1.0187227396915357; eval loss = 1.5707024671137337\n",
      "Epoch 812: train loss = 0.9679101506868999; eval loss = 1.5649792402982707\n",
      "Epoch 813: train loss = 0.9677448204408089; eval loss = 1.558388806879521\n",
      "Epoch 814: train loss = 0.9723721941312155; eval loss = 1.570558945337932\n",
      "Epoch 815: train loss = 0.9720843806862831; eval loss = 1.568587888032198\n",
      "Epoch 816: train loss = 0.9673314255972705; eval loss = 1.5153753521541748\n",
      "Epoch 817: train loss = 0.9860499814773599; eval loss = 1.5445410907268515\n",
      "Epoch 818: train loss = 0.9631282997628056; eval loss = 1.5431065931916224\n",
      "Epoch 819: train loss = 0.9658769701297083; eval loss = 1.5500039681792255\n",
      "Epoch 820: train loss = 0.9727363828569652; eval loss = 1.4899229407310484\n",
      "Epoch 821: train loss = 0.9397494855026405; eval loss = 1.4845446199178707\n",
      "Epoch 822: train loss = 0.9321707813069222; eval loss = 1.4552164822816838\n",
      "Epoch 823: train loss = 0.9331026350458461; eval loss = 1.4515733122825623\n",
      "Epoch 824: train loss = 0.9555526354039707; eval loss = 1.429091189056635\n",
      "Epoch 825: train loss = 0.9466759506613017; eval loss = 1.4289930798113355\n",
      "Epoch 826: train loss = 0.9436865833898386; eval loss = 1.4134298115968704\n",
      "Epoch 827: train loss = 0.9366127007330459; eval loss = 1.3980601653456688\n",
      "Epoch 828: train loss = 0.9407172172019879; eval loss = 1.370931424200535\n",
      "Epoch 829: train loss = 0.9497372144833209; eval loss = 1.746082544326783\n",
      "Epoch 830: train loss = 0.9241681235531966; eval loss = 1.6849428787827483\n",
      "Epoch 831: train loss = 0.93618455187728; eval loss = 1.6928243823349474\n",
      "Epoch 832: train loss = 0.9365567406639458; eval loss = 1.610292721539736\n",
      "Epoch 833: train loss = 0.9607377784947552; eval loss = 1.5259544141590602\n",
      "Epoch 834: train loss = 0.956575843816002; eval loss = 1.4725211896002304\n",
      "Epoch 835: train loss = 0.937584945311149; eval loss = 1.4841270260512822\n",
      "Epoch 836: train loss = 0.8984999939178425; eval loss = 1.4697909113019718\n",
      "Epoch 837: train loss = 0.9098745969434578; eval loss = 1.4089173190295687\n",
      "Epoch 838: train loss = 0.8959030319626134; eval loss = 1.4237358681857581\n",
      "Epoch 839: train loss = 0.9083344194417197; eval loss = 1.400665726512671\n",
      "Epoch 840: train loss = 0.924673297442496; eval loss = 1.3852331954985848\n",
      "Epoch 841: train loss = 0.9195973497504993; eval loss = 1.8524650186300284\n",
      "Epoch 842: train loss = 0.944824941456318; eval loss = 1.3742033019661908\n",
      "Epoch 843: train loss = 0.9109534068653983; eval loss = 1.7693014144897452\n",
      "Epoch 844: train loss = 0.9019421854366859; eval loss = 1.7873764857649812\n",
      "Epoch 845: train loss = 0.9064730657264589; eval loss = 1.4305764362215996\n",
      "Epoch 846: train loss = 0.9159152178714673; eval loss = 1.7045530751347535\n",
      "Epoch 847: train loss = 0.9441381410385172; eval loss = 1.519992381334306\n",
      "Epoch 848: train loss = 0.9649751667554182; eval loss = 1.463073898106813\n",
      "Epoch 849: train loss = 0.9608674931029481; eval loss = 1.4050178900361057\n",
      "Epoch 850: train loss = 0.9724730948607129; eval loss = 1.4162203632295134\n",
      "Epoch 851: train loss = 0.9205619640027481; eval loss = 1.4259137287735937\n",
      "Epoch 852: train loss = 0.9285693289712069; eval loss = 1.423620074987412\n",
      "Epoch 853: train loss = 0.9021796217809118; eval loss = 1.4014398753643047\n",
      "Epoch 854: train loss = 0.9097036157424249; eval loss = 1.4818578362464905\n",
      "Epoch 855: train loss = 0.9200016319130858; eval loss = 1.5661620683968058\n",
      "Epoch 856: train loss = 0.9379183361306784; eval loss = 1.3581851385533816\n",
      "Epoch 857: train loss = 0.9232123028486966; eval loss = 1.3593314550817008\n",
      "Epoch 858: train loss = 0.9325713378687701; eval loss = 1.3422750793397424\n",
      "Epoch 859: train loss = 0.9373939748232565; eval loss = 1.3171223178505902\n",
      "Epoch 860: train loss = 0.9264774285256863; eval loss = 1.3606746494770052\n",
      "Epoch 861: train loss = 0.9270974850902955; eval loss = 1.3546153530478477\n",
      "Epoch 862: train loss = 0.9438953024024765; eval loss = 1.2614202275872233\n",
      "Epoch 863: train loss = 0.9192222139487665; eval loss = 1.4291602298617363\n",
      "Epoch 864: train loss = 0.9035717990870279; eval loss = 1.3916665203869347\n",
      "Epoch 865: train loss = 0.8961783287425837; eval loss = 1.3952706791460514\n",
      "Epoch 866: train loss = 0.890543969658514; eval loss = 1.3416835516691212\n",
      "Epoch 867: train loss = 0.9072612803429366; eval loss = 1.5498928315937526\n",
      "Epoch 868: train loss = 0.9213852950682244; eval loss = 1.2991301640868187\n",
      "Epoch 869: train loss = 0.9078179392963648; eval loss = 1.3038944974541673\n",
      "Epoch 870: train loss = 0.9142188041781387; eval loss = 1.3050924539566053\n",
      "Epoch 871: train loss = 0.9137545718501012; eval loss = 1.330639146268368\n",
      "Epoch 872: train loss = 0.9098027913520734; eval loss = 1.6812446340918532\n",
      "Epoch 873: train loss = 0.9180654662971698; eval loss = 1.5708440542221056\n",
      "Epoch 874: train loss = 0.9238149784505366; eval loss = 1.2807322740554812\n",
      "Epoch 875: train loss = 0.9462980770816405; eval loss = 1.301551796495915\n",
      "Epoch 876: train loss = 0.9501874682803948; eval loss = 1.3132138438522811\n",
      "Epoch 877: train loss = 0.9664230073491733; eval loss = 1.23854448646307\n",
      "Epoch 878: train loss = 0.9486187743023041; eval loss = 1.2281056903302676\n",
      "Epoch 879: train loss = 0.9318120069801805; eval loss = 1.4255892559885976\n",
      "Epoch 880: train loss = 0.9538950975984335; eval loss = 1.3173464760184295\n",
      "Epoch 881: train loss = 0.9031245457008479; eval loss = 1.5209346115589126\n",
      "Epoch 882: train loss = 0.9219314623624089; eval loss = 1.3090409189462657\n",
      "Epoch 883: train loss = 0.9162684439991909; eval loss = 1.672719419002532\n",
      "Epoch 884: train loss = 0.9217078542957705; eval loss = 1.3184189759194842\n",
      "Epoch 885: train loss = 0.9493570104241371; eval loss = 1.2906014248728748\n",
      "Epoch 886: train loss = 0.9553721152866881; eval loss = 1.3943449594080444\n",
      "Epoch 887: train loss = 0.9825514396652578; eval loss = 1.3404381349682806\n",
      "Epoch 888: train loss = 0.9746050449709097; eval loss = 1.2853333912789806\n",
      "Epoch 889: train loss = 0.9723657279585799; eval loss = 1.343885362148286\n",
      "Epoch 890: train loss = 0.9362514587119223; eval loss = 1.298403378576041\n",
      "Epoch 891: train loss = 0.942666939459741; eval loss = 1.2417918145656588\n",
      "Epoch 892: train loss = 0.9624635738631089; eval loss = 1.5578222870826732\n",
      "Epoch 893: train loss = 0.9572460663815338; eval loss = 1.2987042851746071\n",
      "Epoch 894: train loss = 0.980300223454833; eval loss = 1.2640345804393296\n",
      "Epoch 895: train loss = 0.9785413537174465; eval loss = 1.4369761943817143\n",
      "Epoch 896: train loss = 0.9543705390145378; eval loss = 1.3062324374914176\n",
      "Epoch 897: train loss = 0.9310946324840187; eval loss = 1.3694405257701867\n",
      "Epoch 898: train loss = 0.9474768781413633; eval loss = 1.2423454485833652\n",
      "Epoch 899: train loss = 0.9284032092740141; eval loss = 1.1969633288681505\n",
      "Epoch 900: train loss = 0.9909197191397348; eval loss = 1.305543411523104\n",
      "Epoch 901: train loss = 0.9252825758109491; eval loss = 1.3012384772300722\n",
      "Epoch 902: train loss = 0.9030124989027776; eval loss = 1.4042282178998005\n",
      "Epoch 903: train loss = 0.949743738087515; eval loss = 1.2137170694768422\n",
      "Epoch 904: train loss = 0.9724605589484173; eval loss = 1.2008733600378036\n",
      "Epoch 905: train loss = 0.9695881235723695; eval loss = 1.1918228007853036\n",
      "Epoch 906: train loss = 0.9759943432485062; eval loss = 1.190868057310581\n",
      "Epoch 907: train loss = 0.9655090142041446; eval loss = 1.3167130835354335\n",
      "Epoch 908: train loss = 0.9209460867568849; eval loss = 1.264811515808106\n",
      "Epoch 909: train loss = 0.924384114642938; eval loss = 1.286926902830601\n",
      "Epoch 910: train loss = 0.914898894416789; eval loss = 1.2753234282135968\n",
      "Epoch 911: train loss = 0.9289280899489917; eval loss = 1.2564464658498764\n",
      "Epoch 912: train loss = 0.9232015820840994; eval loss = 1.289004627615213\n",
      "Epoch 913: train loss = 0.9079297423983614; eval loss = 1.3114316463470461\n",
      "Epoch 914: train loss = 0.9091690542797246; eval loss = 1.2607553042471404\n",
      "Epoch 915: train loss = 0.91035795211792; eval loss = 1.2358832806348803\n",
      "Epoch 916: train loss = 0.9141811092073717; eval loss = 1.207931753247976\n",
      "Epoch 917: train loss = 0.9039202102770408; eval loss = 1.3816431462764733\n",
      "Epoch 918: train loss = 0.9899053672949473; eval loss = 1.430486857891083\n",
      "Epoch 919: train loss = 0.9336382128919164; eval loss = 1.2125024870038035\n",
      "Epoch 920: train loss = 0.9104563395182291; eval loss = 1.1552887819707394\n",
      "Epoch 921: train loss = 0.9009021104623876; eval loss = 1.2477987930178642\n",
      "Epoch 922: train loss = 0.9457237304498751; eval loss = 1.204137783497572\n",
      "Epoch 923: train loss = 0.984646236213545; eval loss = 1.1591393500566494\n",
      "Epoch 924: train loss = 0.9977167050043743; eval loss = 1.1998585648834714\n",
      "Epoch 925: train loss = 0.9922088331853349; eval loss = 1.2344904318451884\n",
      "Epoch 926: train loss = 1.0084790612260504; eval loss = 1.1593683809041986\n",
      "Epoch 927: train loss = 1.0078315576538444; eval loss = 1.1514527350664148\n",
      "Epoch 928: train loss = 0.9950984927515188; eval loss = 1.19538327306509\n",
      "Epoch 929: train loss = 0.9486050323272744; eval loss = 1.3738572150468815\n",
      "Epoch 930: train loss = 0.9629209966709216; eval loss = 1.2038041502237324\n",
      "Epoch 931: train loss = 0.940857454203069; eval loss = 1.1620595939457414\n",
      "Epoch 932: train loss = 0.9390234627450506; eval loss = 1.1587657816708092\n",
      "Epoch 933: train loss = 0.925556988765796; eval loss = 1.446964353322982\n",
      "Epoch 934: train loss = 0.9762706275408465; eval loss = 1.4293068498373027\n",
      "Epoch 935: train loss = 0.9648953350260853; eval loss = 1.4373438134789471\n",
      "Epoch 936: train loss = 0.9367978970209756; eval loss = 1.362171441316605\n",
      "Epoch 937: train loss = 0.958493048635622; eval loss = 1.2670605219900608\n",
      "Epoch 938: train loss = 0.9424100512017806; eval loss = 1.2589934021234515\n",
      "Epoch 939: train loss = 0.9389914087951184; eval loss = 1.3012744225561623\n",
      "Epoch 940: train loss = 0.9506798936054112; eval loss = 1.3503515087068079\n",
      "Epoch 941: train loss = 0.9320705132558941; eval loss = 1.3145008347928522\n",
      "Epoch 942: train loss = 0.9315672647207974; eval loss = 1.3168385277191805\n",
      "Epoch 943: train loss = 0.9310059190417329; eval loss = 1.324124722431104\n",
      "Epoch 944: train loss = 0.9292292377601066; eval loss = 1.3134280629456045\n",
      "Epoch 945: train loss = 0.9304465012003977; eval loss = 1.3163239645461249\n",
      "Epoch 946: train loss = 0.9296029253552359; eval loss = 1.3116776421666139\n",
      "Epoch 947: train loss = 0.9339222110187015; eval loss = 1.3161769323050976\n",
      "Epoch 948: train loss = 0.9319814974442124; eval loss = 1.3123758435249337\n",
      "Epoch 949: train loss = 0.9324442840491731; eval loss = 1.3119694069027892\n",
      "Epoch 950: train loss = 0.9316784236580136; eval loss = 1.3156700097024436\n",
      "Epoch 951: train loss = 0.9300509483243028; eval loss = 1.3238812349736682\n",
      "Epoch 952: train loss = 0.9305262363826234; eval loss = 1.3366305579741795\n",
      "Epoch 953: train loss = 0.9282756873096031; eval loss = 1.3284259438514712\n",
      "Epoch 954: train loss = 0.929280542458097; eval loss = 1.3333540521562108\n",
      "Epoch 955: train loss = 0.9291997148344915; eval loss = 1.3316296339035039\n",
      "Epoch 956: train loss = 0.9405752305562296; eval loss = 1.2677743993699555\n",
      "Epoch 957: train loss = 0.929845202403764; eval loss = 1.2995692752301689\n",
      "Epoch 958: train loss = 0.9382880028958124; eval loss = 1.2636186741292477\n",
      "Epoch 959: train loss = 0.9365887114157279; eval loss = 1.3247063234448442\n",
      "Epoch 960: train loss = 0.9316757808749874; eval loss = 1.303804431110621\n",
      "Epoch 961: train loss = 0.9363514346381029; eval loss = 1.3040740005671978\n",
      "Epoch 962: train loss = 0.9378543182586632; eval loss = 1.2877981364727016\n",
      "Epoch 963: train loss = 0.9424904230982065; eval loss = 1.2768066413700587\n",
      "Epoch 964: train loss = 0.9405187498778103; eval loss = 1.2858408652246003\n",
      "Epoch 965: train loss = 1.0084680132567883; eval loss = 1.3581460528075686\n",
      "Epoch 966: train loss = 1.0414595063775778; eval loss = 1.3511957712471485\n",
      "Epoch 967: train loss = 1.0063770000512402; eval loss = 1.290921837091446\n",
      "Epoch 968: train loss = 1.0216635434577865; eval loss = 1.2733091525733473\n",
      "Epoch 969: train loss = 1.0111687695607543; eval loss = 1.4682138115167616\n",
      "Epoch 970: train loss = 0.9964055158197881; eval loss = 1.3843579515814781\n",
      "Epoch 971: train loss = 0.9935013018548489; eval loss = 1.265972465276719\n",
      "Epoch 972: train loss = 1.000560876913369; eval loss = 1.2470410317182532\n",
      "Epoch 973: train loss = 0.9668424992511669; eval loss = 1.6841571219265463\n",
      "Epoch 974: train loss = 1.0140252128864329; eval loss = 1.3678962513804434\n",
      "Epoch 975: train loss = 1.0211429592842856; eval loss = 1.674496877938508\n",
      "Epoch 976: train loss = 1.0430437838658688; eval loss = 1.4271590150892743\n",
      "Epoch 977: train loss = 1.0252574579790237; eval loss = 1.6411461010575292\n",
      "Epoch 978: train loss = 1.0264343911161027; eval loss = 1.424951456487179\n",
      "Epoch 979: train loss = 1.0164140779525042; eval loss = 1.3770015761256227\n",
      "Epoch 980: train loss = 1.0326563104366266; eval loss = 1.325248192995787\n",
      "Epoch 981: train loss = 1.0101929139345884; eval loss = 1.8624672181904323\n",
      "Epoch 982: train loss = 1.0048246833806234; eval loss = 1.6710629090666766\n",
      "Epoch 983: train loss = 0.9714173059910539; eval loss = 1.5426787100732315\n",
      "Epoch 984: train loss = 0.9502410677572095; eval loss = 1.4701157733798018\n",
      "Epoch 985: train loss = 0.9301533255105217; eval loss = 1.460808135569095\n",
      "Epoch 986: train loss = 0.9269101532797019; eval loss = 1.4538503102958196\n",
      "Epoch 987: train loss = 0.9211152624338864; eval loss = 1.437940638512374\n",
      "Epoch 988: train loss = 0.923060821990172; eval loss = 1.4249651581048977\n",
      "Epoch 989: train loss = 0.9258852340281011; eval loss = 1.4262214675545701\n",
      "Epoch 990: train loss = 0.9217774259547393; eval loss = 1.596174899488688\n",
      "Epoch 991: train loss = 1.0267106012130778; eval loss = 1.504646372050047\n",
      "Epoch 992: train loss = 1.0368863393863041; eval loss = 1.4944885708391666\n",
      "Epoch 993: train loss = 1.036942976526916; eval loss = 1.4891160391271114\n",
      "Epoch 994: train loss = 1.0314210830256345; eval loss = 1.653924591839314\n",
      "Epoch 995: train loss = 0.9986850665882229; eval loss = 1.7065530344843867\n",
      "Epoch 996: train loss = 0.9991176153222721; eval loss = 1.5753276348114005\n",
      "Epoch 997: train loss = 0.9923914149403573; eval loss = 1.8270755931735034\n",
      "Epoch 998: train loss = 0.9816577043384314; eval loss = 1.8444923758506775\n",
      "Epoch 999: train loss = 0.9644998389606673; eval loss = 1.7994642779231071\n",
      "Epoch 1000: train loss = 0.9660242289925611; eval loss = 1.5489250496029858\n",
      "Epoch 1001: train loss = 0.9759261924773455; eval loss = 1.8937754780054095\n",
      "Epoch 1002: train loss = 0.9708789673944315; eval loss = 1.8756739869713783\n",
      "Epoch 1003: train loss = 0.9658929255480567; eval loss = 1.6041224598884583\n",
      "Epoch 1004: train loss = 0.9505848651751877; eval loss = 1.571157556027174\n",
      "Epoch 1005: train loss = 0.9485015378644067; eval loss = 1.5494681745767598\n",
      "Epoch 1006: train loss = 0.9465440201262635; eval loss = 1.5294820256531243\n",
      "Epoch 1007: train loss = 0.9545579108720025; eval loss = 1.5145098157227044\n",
      "Epoch 1008: train loss = 0.9552718199168645; eval loss = 1.5040948241949077\n",
      "Epoch 1009: train loss = 0.955000293130676; eval loss = 1.4726095646619806\n",
      "Epoch 1010: train loss = 0.9647285345320901; eval loss = 1.8594265803694723\n",
      "Epoch 1011: train loss = 0.9665231940646964; eval loss = 1.4884944967925553\n",
      "Epoch 1012: train loss = 0.9378721745063862; eval loss = 1.4990766309201717\n",
      "Epoch 1013: train loss = 0.9308569449931385; eval loss = 1.492425799369812\n",
      "Epoch 1014: train loss = 0.928200273774564; eval loss = 1.4721827693283556\n",
      "Epoch 1015: train loss = 0.9268738956501086; eval loss = 1.474154401570559\n",
      "Epoch 1016: train loss = 0.9216165402904154; eval loss = 1.4546070843935006\n",
      "Epoch 1017: train loss = 0.9211442253241935; eval loss = 1.4543340466916572\n",
      "Epoch 1018: train loss = 0.9171625217422843; eval loss = 1.4420719221234322\n",
      "Epoch 1019: train loss = 0.915285630772511; eval loss = 1.4366648718714705\n",
      "Epoch 1020: train loss = 0.9134485833346843; eval loss = 1.4288673251867305\n",
      "Epoch 1021: train loss = 0.9123212366054456; eval loss = 1.4222635775804513\n",
      "Epoch 1022: train loss = 0.9096056319152318; eval loss = 1.4151595160365105\n",
      "Epoch 1023: train loss = 0.9067344171926379; eval loss = 1.4128045998513699\n",
      "Epoch 1024: train loss = 0.906106416446467; eval loss = 1.4124082146833334\n",
      "Epoch 1025: train loss = 0.9048636518418789; eval loss = 1.404411570479473\n",
      "Epoch 1026: train loss = 0.9025339682896933; eval loss = 1.4078625291585924\n",
      "Epoch 1027: train loss = 0.9017120062684022; eval loss = 1.4050146490335473\n",
      "Epoch 1028: train loss = 0.8995845721413694; eval loss = 1.40222380310297\n",
      "Epoch 1029: train loss = 0.8973973756656051; eval loss = 1.3964108514289073\n",
      "Epoch 1030: train loss = 0.8964338293299079; eval loss = 1.3884687647223475\n",
      "Epoch 1031: train loss = 0.9293491731708248; eval loss = 1.3153220613797507\n",
      "Epoch 1032: train loss = 0.9408265029390651; eval loss = 1.297109182924031\n",
      "Epoch 1033: train loss = 0.952080431083838; eval loss = 1.3327725678682336\n",
      "Epoch 1034: train loss = 0.9234276041388512; eval loss = 1.4419418312609198\n",
      "Epoch 1035: train loss = 0.9549900501345594; eval loss = 1.4119347818195822\n",
      "Epoch 1036: train loss = 0.93639883492142; eval loss = 1.2548891815046472\n",
      "Epoch 1037: train loss = 0.9630739099035658; eval loss = 1.2973478436470036\n",
      "Epoch 1038: train loss = 0.97849663409094; eval loss = 1.261224549263716\n",
      "Epoch 1039: train loss = 0.99890742264688; eval loss = 1.2905300307486742\n",
      "Epoch 1040: train loss = 0.9402896305546166; eval loss = 1.5310565680265424\n",
      "Epoch 1041: train loss = 0.9835136489321787; eval loss = 1.4999555200338361\n",
      "Epoch 1042: train loss = 0.9233425995334983; eval loss = 1.4932968989014634\n",
      "Epoch 1043: train loss = 1.0019820351153614; eval loss = 1.3952513337135308\n",
      "Epoch 1044: train loss = 1.0119164477412903; eval loss = 1.3537900745868683\n",
      "Epoch 1045: train loss = 0.9995660306885837; eval loss = 1.6549108140170565\n",
      "Epoch 1046: train loss = 0.9787176921963692; eval loss = 1.7566174529492862\n",
      "Epoch 1047: train loss = 0.9785765822355943; eval loss = 1.5586062371730793\n",
      "Epoch 1048: train loss = 0.9730415508771937; eval loss = 1.4559174068272114\n",
      "Epoch 1049: train loss = 1.0263669950266683; eval loss = 1.5052426271140573\n",
      "Epoch 1050: train loss = 0.9688553037121891; eval loss = 1.5197898708283897\n",
      "Epoch 1051: train loss = 0.9615097750599187; eval loss = 1.5141638144850735\n",
      "Epoch 1052: train loss = 0.9633016334846614; eval loss = 1.5002077519893635\n",
      "Epoch 1053: train loss = 0.9679925888776778; eval loss = 1.4899288713932035\n",
      "Epoch 1054: train loss = 0.9674440591285627; eval loss = 1.5398494750261305\n",
      "Epoch 1055: train loss = 1.0239850422367454; eval loss = 1.4128245115280158\n",
      "Epoch 1056: train loss = 1.0012261932715774; eval loss = 1.4043372906744482\n",
      "Epoch 1057: train loss = 0.9785667692000668; eval loss = 1.4189864806830883\n",
      "Epoch 1058: train loss = 0.9388503655791284; eval loss = 1.5096883177757252\n",
      "Epoch 1059: train loss = 0.9460398480296134; eval loss = 1.5371445938944814\n",
      "Epoch 1060: train loss = 0.9475862520436448; eval loss = 1.5108463428914545\n",
      "Epoch 1061: train loss = 0.9462497880061467; eval loss = 1.5049756094813345\n",
      "Epoch 1062: train loss = 0.9533869170894225; eval loss = 1.4896144854525721\n",
      "Epoch 1063: train loss = 0.9518895282720526; eval loss = 1.4980626814067357\n",
      "Epoch 1064: train loss = 0.9507597473760445; eval loss = 1.5011506378650656\n",
      "Epoch 1065: train loss = 0.9567892355844378; eval loss = 1.5086329542100443\n",
      "Epoch 1066: train loss = 0.9650518456473945; eval loss = 1.5041744299232955\n",
      "Epoch 1067: train loss = 0.9779744228969015; eval loss = 1.5719324909150607\n",
      "Epoch 1068: train loss = 1.017275668059786; eval loss = 1.573323465883732\n",
      "Epoch 1069: train loss = 0.9592992976928749; eval loss = 1.48718074336648\n",
      "Epoch 1070: train loss = 0.9738662286351124; eval loss = 1.4774136543273917\n",
      "Epoch 1071: train loss = 0.9841332674647371; eval loss = 1.4634783565998075\n",
      "Epoch 1072: train loss = 0.9972931199396652; eval loss = 1.4480087161064137\n",
      "Epoch 1073: train loss = 0.9781255843117831; eval loss = 1.4212381765246387\n",
      "Epoch 1074: train loss = 0.9830047553405166; eval loss = 1.765787046402693\n",
      "Epoch 1075: train loss = 0.9913357713570199; eval loss = 1.402516353875399\n",
      "Epoch 1076: train loss = 0.9933109569052854; eval loss = 1.3078689761459836\n",
      "Epoch 1077: train loss = 0.9974360953395567; eval loss = 1.5527185238897805\n",
      "Epoch 1078: train loss = 0.9923116844147443; eval loss = 1.417964827269316\n",
      "Epoch 1079: train loss = 0.9381940259287755; eval loss = 1.3908949121832845\n",
      "Epoch 1080: train loss = 0.9356563507268827; eval loss = 1.3923509493470183\n",
      "Epoch 1081: train loss = 0.9345688285926977; eval loss = 1.3926144465804107\n",
      "Epoch 1082: train loss = 0.9338561892509459; eval loss = 1.39299688115716\n",
      "Epoch 1083: train loss = 0.9312787471959988; eval loss = 1.3924838230013854\n",
      "Epoch 1084: train loss = 0.9303111123541992; eval loss = 1.3925955817103384\n",
      "Epoch 1085: train loss = 0.9298444529995321; eval loss = 1.3911930583417411\n",
      "Epoch 1086: train loss = 0.9294759708767135; eval loss = 1.3905744031071658\n",
      "Epoch 1087: train loss = 0.9290909490858519; eval loss = 1.389859523624181\n",
      "Epoch 1088: train loss = 0.9302931623533371; eval loss = 1.3878113888204098\n",
      "Epoch 1089: train loss = 0.9286887925118206; eval loss = 1.3890988156199453\n",
      "Epoch 1090: train loss = 0.9289298889537655; eval loss = 1.3922492116689682\n",
      "Epoch 1091: train loss = 0.929688918404281; eval loss = 1.3888540826737878\n",
      "Epoch 1092: train loss = 0.9309174964825313; eval loss = 1.3866604752838614\n",
      "Epoch 1093: train loss = 0.9344784157971544; eval loss = 1.3922060802578928\n",
      "Epoch 1094: train loss = 0.9348591674740117; eval loss = 1.3979576192796228\n",
      "Epoch 1095: train loss = 0.9340916698177656; eval loss = 1.4047185704112055\n",
      "Epoch 1096: train loss = 0.9341776591415207; eval loss = 1.41122006252408\n",
      "Epoch 1097: train loss = 0.931149147450924; eval loss = 1.427767537534236\n",
      "Epoch 1098: train loss = 0.930167856005331; eval loss = 1.4285040944814682\n",
      "Epoch 1099: train loss = 0.9294614940881726; eval loss = 1.4289158433675768\n",
      "Epoch 1100: train loss = 0.9306088568021853; eval loss = 1.435642451047897\n",
      "Epoch 1101: train loss = 0.9298984979589778; eval loss = 1.4575853161513808\n",
      "Epoch 1102: train loss = 0.926562155286471; eval loss = 1.4566047415137282\n",
      "Epoch 1103: train loss = 0.9253274717678626; eval loss = 1.4645890295505521\n",
      "Epoch 1104: train loss = 0.9225082652022445; eval loss = 1.4852360114455225\n",
      "Epoch 1105: train loss = 0.9253761501361926; eval loss = 1.4748379923403263\n",
      "Epoch 1106: train loss = 0.9301750271891556; eval loss = 1.4198313504457478\n",
      "Epoch 1107: train loss = 0.9303973400965334; eval loss = 1.4458475895226002\n",
      "Epoch 1108: train loss = 0.9364064205437895; eval loss = 1.5998709065218775\n",
      "Epoch 1109: train loss = 1.0006919673954449; eval loss = 1.4032259844243522\n",
      "Epoch 1110: train loss = 1.0042879031971095; eval loss = 1.390666466206312\n",
      "Epoch 1111: train loss = 1.0167007477333148; eval loss = 1.388482349614302\n",
      "Epoch 1112: train loss = 1.0330813179413478; eval loss = 1.4070183560252187\n",
      "Epoch 1113: train loss = 1.0205522825320563; eval loss = 1.6677851006388664\n",
      "Epoch 1114: train loss = 0.9877487992246946; eval loss = 1.7251810431480412\n",
      "Epoch 1115: train loss = 0.9683582143237192; eval loss = 1.5210826545953744\n",
      "Epoch 1116: train loss = 0.945350934751332; eval loss = 1.4736136272549627\n",
      "Epoch 1117: train loss = 0.9399921965474884; eval loss = 1.475629512220621\n",
      "Epoch 1118: train loss = 0.9407212808728223; eval loss = 1.4578415751457205\n",
      "Epoch 1119: train loss = 0.9510841217512886; eval loss = 1.4349965117871764\n",
      "Epoch 1120: train loss = 0.9487405692537626; eval loss = 1.4538373500108719\n",
      "Epoch 1121: train loss = 0.9660269863282641; eval loss = 1.3657466582953934\n",
      "Epoch 1122: train loss = 0.9611268791680534; eval loss = 1.3891288526356225\n",
      "Epoch 1123: train loss = 0.9592936035866539; eval loss = 1.3825037255883221\n",
      "Epoch 1124: train loss = 0.952625627319018; eval loss = 1.5340148732066157\n",
      "Epoch 1125: train loss = 1.01215416751802; eval loss = 1.6309611275792124\n",
      "Epoch 1126: train loss = 0.93955667099605; eval loss = 1.373564597219229\n",
      "Epoch 1127: train loss = 0.9229671427359182; eval loss = 1.4085592254996304\n",
      "Epoch 1128: train loss = 0.9206744364152353; eval loss = 1.421255219727754\n",
      "Epoch 1129: train loss = 0.919752205722034; eval loss = 1.4109033346176145\n",
      "Epoch 1130: train loss = 0.9215183400859436; eval loss = 1.4130938686430456\n",
      "Epoch 1131: train loss = 0.9195108115673066; eval loss = 1.4165408052504067\n",
      "Epoch 1132: train loss = 0.9203071917096772; eval loss = 1.4188416227698322\n",
      "Epoch 1133: train loss = 0.9213031176477668; eval loss = 1.4223702885210516\n",
      "Epoch 1134: train loss = 0.9205136274298032; eval loss = 1.4226020835340023\n",
      "Epoch 1135: train loss = 0.9205738501623273; eval loss = 1.4278355501592148\n",
      "Epoch 1136: train loss = 0.9203851803516349; eval loss = 1.4307424873113632\n",
      "Epoch 1137: train loss = 0.920832528422276; eval loss = 1.4312411919236177\n",
      "Epoch 1138: train loss = 0.9192928715298573; eval loss = 1.4360894262790682\n",
      "Epoch 1139: train loss = 0.9216397926211354; eval loss = 1.4376789443194864\n",
      "Epoch 1140: train loss = 0.9215154657140374; eval loss = 1.4387189857661724\n",
      "Epoch 1141: train loss = 0.9222002945219477; eval loss = 1.4458809755742557\n",
      "Epoch 1142: train loss = 0.92270299885422; eval loss = 1.4484969191253176\n",
      "Epoch 1143: train loss = 0.9233187704036635; eval loss = 1.4492957703769211\n",
      "Epoch 1144: train loss = 0.9228959285343685; eval loss = 1.4527472183108328\n",
      "Epoch 1145: train loss = 0.9229031996801497; eval loss = 1.453383520245553\n",
      "Epoch 1146: train loss = 0.9266005869333948; eval loss = 1.4488606639206416\n",
      "Epoch 1147: train loss = 0.9277438291658958; eval loss = 1.4422169178724298\n",
      "Epoch 1148: train loss = 0.929289801356693; eval loss = 1.3923342376947403\n",
      "Epoch 1149: train loss = 0.9518261170014741; eval loss = 1.4667158350348481\n",
      "Epoch 1150: train loss = 0.9718383233994243; eval loss = 1.4648224338889122\n",
      "Epoch 1151: train loss = 0.9714960986748338; eval loss = 1.462532460689546\n",
      "Epoch 1152: train loss = 0.9754256866872311; eval loss = 1.4766318090260033\n",
      "Epoch 1153: train loss = 0.9719567941501736; eval loss = 1.7281498759984968\n",
      "Epoch 1154: train loss = 0.9553742830952006; eval loss = 1.5713780969381326\n",
      "Epoch 1155: train loss = 0.9566623624414206; eval loss = 1.8485571146011357\n",
      "Epoch 1156: train loss = 0.9391186364615958; eval loss = 1.553537413477898\n",
      "Epoch 1157: train loss = 1.0467076372976107; eval loss = 1.5442283749580386\n",
      "Epoch 1158: train loss = 0.9657562446470063; eval loss = 1.5504369698464875\n",
      "Epoch 1159: train loss = 0.9660830612604817; eval loss = 1.553415231406689\n",
      "Epoch 1160: train loss = 0.960684794622163; eval loss = 1.562531605362893\n",
      "Epoch 1161: train loss = 0.9815467987209561; eval loss = 1.5620004758238795\n",
      "Epoch 1162: train loss = 0.9596869961048161; eval loss = 1.5632699579000486\n",
      "Epoch 1163: train loss = 0.9535416957611839; eval loss = 1.5737192966043945\n",
      "Epoch 1164: train loss = 0.9595971691111725; eval loss = 1.5768231749534602\n",
      "Epoch 1165: train loss = 0.9572253090639908; eval loss = 1.5828253626823412\n",
      "Epoch 1166: train loss = 0.9783792641634745; eval loss = 1.5011712387204172\n",
      "Epoch 1167: train loss = 0.9891347282876571; eval loss = 1.5178816989064219\n",
      "Epoch 1168: train loss = 0.9586424271886548; eval loss = 1.529998254030943\n",
      "Epoch 1169: train loss = 0.9495391088227431; eval loss = 1.5425574369728565\n",
      "Epoch 1170: train loss = 0.950783315425118; eval loss = 1.5385685488581666\n",
      "Epoch 1171: train loss = 0.9520296069482962; eval loss = 1.540280025452375\n",
      "Epoch 1172: train loss = 0.9481443135688703; eval loss = 1.5379538834094992\n",
      "Epoch 1173: train loss = 0.9459700829659897; eval loss = 1.5382831394672394\n",
      "Epoch 1174: train loss = 0.9462715964764358; eval loss = 1.5360827259719372\n",
      "Epoch 1175: train loss = 0.9450696877514322; eval loss = 1.536719977855682\n",
      "Epoch 1176: train loss = 0.9445561400304235; eval loss = 1.5302547886967655\n",
      "Epoch 1177: train loss = 0.9423445047189793; eval loss = 1.5338445827364928\n",
      "Epoch 1178: train loss = 0.9460443115482728; eval loss = 1.531906519085169\n",
      "Epoch 1179: train loss = 0.9435000643134116; eval loss = 1.5301196984946732\n",
      "Epoch 1180: train loss = 0.9438293570031723; eval loss = 1.53122191503644\n",
      "Epoch 1181: train loss = 0.9415231018016736; eval loss = 1.5243959203362465\n",
      "Epoch 1182: train loss = 0.9416961694757142; eval loss = 1.5245738215744487\n",
      "Epoch 1183: train loss = 0.9396566841751337; eval loss = 1.5138633251190186\n",
      "Epoch 1184: train loss = 0.9365068205321831; eval loss = 1.5184975080192085\n",
      "Epoch 1185: train loss = 0.9366230722516771; eval loss = 1.5074834153056145\n",
      "Epoch 1186: train loss = 0.9344211171070735; eval loss = 1.506341584026814\n",
      "Epoch 1187: train loss = 0.9341103490442038; eval loss = 1.5009000152349472\n",
      "Epoch 1188: train loss = 0.9317133724689486; eval loss = 1.5067152604460725\n",
      "Epoch 1189: train loss = 0.9310008321578305; eval loss = 1.4992884919047356\n",
      "Epoch 1190: train loss = 0.9259120902667444; eval loss = 1.4915059879422194\n",
      "Epoch 1191: train loss = 0.9303034426023562; eval loss = 1.5003267526626591\n",
      "Epoch 1192: train loss = 0.9268056983128191; eval loss = 1.4889583662152288\n",
      "Epoch 1193: train loss = 0.9246182156105837; eval loss = 1.4890195019543166\n",
      "Epoch 1194: train loss = 0.9239977691322566; eval loss = 1.4913653805851936\n",
      "Epoch 1195: train loss = 0.934977201744914; eval loss = 1.4965266436338425\n",
      "Epoch 1196: train loss = 0.9358771412322917; eval loss = 1.4910006113350387\n",
      "Epoch 1197: train loss = 0.9321640798201164; eval loss = 1.4902216978371141\n",
      "Epoch 1198: train loss = 0.9318014628564316; eval loss = 1.4911348894238468\n",
      "Epoch 1199: train loss = 0.9348669387400151; eval loss = 1.4873893745243554\n",
      "Epoch 1200: train loss = 0.9310715210934479; eval loss = 1.48314892873168\n",
      "Epoch 1201: train loss = 0.928864434361458; eval loss = 1.4715085551142697\n",
      "Epoch 1202: train loss = 0.9288766495883465; eval loss = 1.480375614017248\n",
      "Epoch 1203: train loss = 0.9258923648546138; eval loss = 1.4717804541190478\n",
      "Epoch 1204: train loss = 0.928304755439361; eval loss = 1.4765071943402293\n",
      "Epoch 1205: train loss = 0.9298259696612755; eval loss = 1.468834832310677\n",
      "Epoch 1206: train loss = 0.9297696799039841; eval loss = 1.471877440810204\n",
      "Epoch 1207: train loss = 0.9348379950970411; eval loss = 1.4713701009750366\n",
      "Epoch 1208: train loss = 0.929063348720471; eval loss = 1.4652796462178237\n",
      "Epoch 1209: train loss = 0.9315974389513332; eval loss = 1.4649126641452306\n",
      "Epoch 1210: train loss = 0.9341914132237435; eval loss = 1.46389289572835\n",
      "Epoch 1211: train loss = 0.9392586878190438; eval loss = 1.461697075515985\n",
      "Epoch 1212: train loss = 0.9739928438017766; eval loss = 1.4575866796076309\n",
      "Epoch 1213: train loss = 1.0121594027926524; eval loss = 1.4401927515864377\n",
      "Epoch 1214: train loss = 1.0117780435830355; eval loss = 1.440846793353557\n",
      "Epoch 1215: train loss = 1.010359534372886; eval loss = 1.4365354180336012\n",
      "Epoch 1216: train loss = 1.0075068448980649; eval loss = 1.4213762916624553\n",
      "Epoch 1217: train loss = 1.0056845769286156; eval loss = 1.4067925140261643\n",
      "Epoch 1218: train loss = 1.0064901492247977; eval loss = 1.383274171501399\n",
      "Epoch 1219: train loss = 1.0092751750101643; eval loss = 1.57752550765872\n",
      "Epoch 1220: train loss = 0.95203358742098; eval loss = 1.5600701458752144\n",
      "Epoch 1221: train loss = 0.9480358837172388; eval loss = 1.5677457079291346\n",
      "Epoch 1222: train loss = 0.9626446999609471; eval loss = 1.4199911952018742\n",
      "Epoch 1223: train loss = 0.9825566013654072; eval loss = 1.4180634319782255\n",
      "Epoch 1224: train loss = 0.979296086045603; eval loss = 1.417635932564735\n",
      "Epoch 1225: train loss = 0.9790160218253735; eval loss = 1.4062703326344506\n",
      "Epoch 1226: train loss = 0.9873408038790028; eval loss = 1.4027901440858839\n",
      "Epoch 1227: train loss = 0.9863178695862492; eval loss = 2.011949904263019\n",
      "Epoch 1228: train loss = 0.969905374882122; eval loss = 1.4589288830757134\n",
      "Epoch 1229: train loss = 1.0249359365552664; eval loss = 2.110790204256774\n",
      "Epoch 1230: train loss = 0.9828300780306258; eval loss = 1.496154882013797\n",
      "Epoch 1231: train loss = 1.0235401106377444; eval loss = 2.135016191750765\n",
      "Epoch 1232: train loss = 0.9785120071222384; eval loss = 2.0057288222014913\n",
      "Epoch 1233: train loss = 0.9801019902030629; eval loss = 2.0419087260961537\n",
      "Epoch 1234: train loss = 0.9657443631440401; eval loss = 2.0070474557578564\n",
      "Epoch 1235: train loss = 0.9649532704303659; eval loss = 1.7600849382579336\n",
      "Epoch 1236: train loss = 0.9896142308910689; eval loss = 1.7610665708780293\n",
      "Epoch 1237: train loss = 0.9827832464749614; eval loss = 1.780966885387898\n",
      "Epoch 1238: train loss = 0.9475380387157204; eval loss = 1.7707993164658546\n",
      "Epoch 1239: train loss = 0.9535579063619178; eval loss = 1.7699427157640453\n",
      "Epoch 1240: train loss = 0.9533976775904495; eval loss = 1.7644389122724529\n",
      "Epoch 1241: train loss = 0.9510595745717488; eval loss = 1.7699647024273877\n",
      "Epoch 1242: train loss = 0.9534636369595924; eval loss = 1.7721685022115699\n",
      "Epoch 1243: train loss = 0.9553507141148051; eval loss = 1.7775937765836705\n",
      "Epoch 1244: train loss = 0.9571640770882369; eval loss = 1.7806557789444926\n",
      "Epoch 1245: train loss = 0.9540942649667462; eval loss = 1.77398093789816\n",
      "Epoch 1246: train loss = 0.9534904050330323; eval loss = 1.7711796909570696\n",
      "Epoch 1247: train loss = 0.9546486927817265; eval loss = 1.7657293975353234\n",
      "Epoch 1248: train loss = 0.9522517972315352; eval loss = 1.7598749101161975\n",
      "Epoch 1249: train loss = 0.9508084747940303; eval loss = 1.755901545286179\n",
      "Epoch 1250: train loss = 0.9505825343852243; eval loss = 1.7507989692191277\n",
      "Epoch 1251: train loss = 0.9475327956800661; eval loss = 1.7480391263961805\n",
      "Epoch 1252: train loss = 0.9475491624325509; eval loss = 1.739755466580391\n",
      "Epoch 1253: train loss = 0.9469352622206013; eval loss = 1.7356345057487481\n",
      "Epoch 1254: train loss = 0.9432654532914362; eval loss = 1.7315265014767656\n",
      "Epoch 1255: train loss = 0.941878399501244; eval loss = 1.7290142178535461\n",
      "Epoch 1256: train loss = 0.9418352867166198; eval loss = 1.7236608713865278\n",
      "Epoch 1257: train loss = 0.9403724136451881; eval loss = 1.7191914767026897\n",
      "Epoch 1258: train loss = 0.939715833713611; eval loss = 1.7165437936782844\n",
      "Epoch 1259: train loss = 0.9385308952381216; eval loss = 1.7127746716141705\n",
      "Epoch 1260: train loss = 0.9354035637030998; eval loss = 1.7100132405757895\n",
      "Epoch 1261: train loss = 0.936717201024294; eval loss = 1.7047870904207236\n",
      "Epoch 1262: train loss = 0.935692881544431; eval loss = 1.699975803494453\n",
      "Epoch 1263: train loss = 0.9384374283254147; eval loss = 1.6922538429498686\n",
      "Epoch 1264: train loss = 0.934383846819401; eval loss = 1.6892795562744147\n",
      "Epoch 1265: train loss = 0.9350055704514184; eval loss = 1.687223158776761\n",
      "Epoch 1266: train loss = 0.9340532403439286; eval loss = 1.6843109987676146\n",
      "Epoch 1267: train loss = 0.9334112368524076; eval loss = 1.6861381642520423\n",
      "Epoch 1268: train loss = 0.9337029177695513; eval loss = 1.684286765754224\n",
      "Epoch 1269: train loss = 0.9345264018823704; eval loss = 1.6793375872075567\n",
      "Epoch 1270: train loss = 0.9321661200374364; eval loss = 1.6767583563923836\n",
      "Epoch 1271: train loss = 0.9342622955640156; eval loss = 1.6725308969616883\n",
      "Epoch 1272: train loss = 0.9333140477538109; eval loss = 1.6702801473438735\n",
      "Epoch 1273: train loss = 0.9326338525861502; eval loss = 1.6677138805389402\n",
      "Epoch 1274: train loss = 0.9338711891323327; eval loss = 1.662803627550602\n",
      "Epoch 1275: train loss = 0.9328526997317873; eval loss = 1.667544410874446\n",
      "Epoch 1276: train loss = 0.937687856455644; eval loss = 1.6645465828478336\n",
      "Epoch 1277: train loss = 0.936959806829691; eval loss = 1.6670417537291857\n",
      "Epoch 1278: train loss = 0.9369097265104454; eval loss = 1.6664253026247016\n",
      "Epoch 1279: train loss = 0.939471419279774; eval loss = 1.6668027788400657\n",
      "Epoch 1280: train loss = 0.9413072088112433; eval loss = 1.6625788547098628\n",
      "Epoch 1281: train loss = 0.9438656034568946; eval loss = 1.6527067571878418\n",
      "Epoch 1282: train loss = 0.9407246963431439; eval loss = 1.613183677196503\n",
      "Epoch 1283: train loss = 0.9521697107702496; eval loss = 1.6738215088844293\n",
      "Epoch 1284: train loss = 0.9390570810064672; eval loss = 1.614647272974252\n",
      "Epoch 1285: train loss = 0.9291141250481209; eval loss = 1.6166254356503489\n",
      "Epoch 1286: train loss = 0.9303465684254966; eval loss = 1.6066591516137128\n",
      "Epoch 1287: train loss = 0.9337888698404034; eval loss = 1.681036747992039\n",
      "Epoch 1288: train loss = 0.9417009502649307; eval loss = 1.5679644010961051\n",
      "Epoch 1289: train loss = 0.9384038721521697; eval loss = 1.5501073226332664\n",
      "Epoch 1290: train loss = 0.938585056612889; eval loss = 1.5523328743875024\n",
      "Epoch 1291: train loss = 0.9489489756524563; eval loss = 1.535851161926984\n",
      "Epoch 1292: train loss = 0.9449404086917639; eval loss = 1.5310820005834112\n",
      "Epoch 1293: train loss = 0.9495316967368125; eval loss = 1.5178158953785887\n",
      "Epoch 1294: train loss = 0.9446997400373223; eval loss = 1.4976724088191993\n",
      "Epoch 1295: train loss = 0.9448452970633905; eval loss = 1.4988076649606228\n",
      "Epoch 1296: train loss = 0.9371411725878716; eval loss = 1.4953789673745632\n",
      "Epoch 1297: train loss = 0.9332249655077858; eval loss = 1.4881825484335438\n",
      "Epoch 1298: train loss = 0.9388778582215311; eval loss = 1.4695976302027693\n",
      "Epoch 1299: train loss = 0.9421962983906269; eval loss = 1.464929811656475\n",
      "Epoch 1300: train loss = 0.9460521129270395; eval loss = 1.4619317539036278\n",
      "Epoch 1301: train loss = 0.9343568837891022; eval loss = 1.4224195852875698\n",
      "Epoch 1302: train loss = 0.948607056712111; eval loss = 1.4400749802589423\n",
      "Epoch 1303: train loss = 0.9599235694234571; eval loss = 1.3920448720455176\n",
      "Epoch 1304: train loss = 0.9659789158031342; eval loss = 1.450346570461988\n",
      "Epoch 1305: train loss = 0.9399052367856108; eval loss = 1.4816480353474624\n",
      "Epoch 1306: train loss = 0.934241441388925; eval loss = 1.4790367459257439\n",
      "Epoch 1307: train loss = 0.9389436164249976; eval loss = 1.480130273848772\n",
      "Epoch 1308: train loss = 0.9375951296339433; eval loss = 1.4848372787237165\n",
      "Epoch 1309: train loss = 0.9379796193291744; eval loss = 1.4854906611144545\n",
      "Epoch 1310: train loss = 0.9374767237653336; eval loss = 1.4883346259593966\n",
      "Epoch 1311: train loss = 0.9345588559905688; eval loss = 1.4893534332513811\n",
      "Epoch 1312: train loss = 0.937724534422159; eval loss = 1.4947207719087607\n",
      "Epoch 1313: train loss = 0.9383053028335174; eval loss = 1.4933537617325783\n",
      "Epoch 1314: train loss = 0.931497006987532; eval loss = 1.4947574846446514\n",
      "Epoch 1315: train loss = 0.931143815939625; eval loss = 1.4987586662173267\n",
      "Epoch 1316: train loss = 0.9316943958401678; eval loss = 1.500853538513184\n",
      "Epoch 1317: train loss = 0.9291162807494402; eval loss = 1.4985041469335558\n",
      "Epoch 1318: train loss = 0.9294658210128545; eval loss = 1.5010177046060562\n",
      "Epoch 1319: train loss = 0.9286370103557906; eval loss = 1.502511959522963\n",
      "Epoch 1320: train loss = 0.9257586579769852; eval loss = 1.5019455887377269\n",
      "Epoch 1321: train loss = 0.923158623278141; eval loss = 1.501505374908447\n",
      "Epoch 1322: train loss = 0.9239262752234934; eval loss = 1.4991292394697666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(test_loader\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m()):\n\u001b[1;32m     28\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\n\u001b[0;32m---> 29\u001b[0m         eval_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m test_loader\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     eval_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model\u001b[38;5;241m=\u001b[39mmodel, data\u001b[38;5;241m=\u001b[39mdata, args\u001b[38;5;241m=\u001b[39margs)\n",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m, in \u001b[0;36meval\u001b[0;34m(model, data, args)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 48\u001b[0m     F_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF_0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeshfield\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeshfield\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_time\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[1;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mcriterion(F_pred, F_true)\n",
      "File \u001b[0;32m/data1/tam/python_graph_utilities_v3/Run/../Codes_1Dtree/networks/gcnv5.py:123\u001b[0m, in \u001b[0;36mRecurrentFormulationNet.forward\u001b[0;34m(self, F_0, edge_index, meshfield, n_time, device)\u001b[0m\n\u001b[1;32m    120\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput(x\u001b[38;5;241m=\u001b[39mnode_attr, edge_index\u001b[38;5;241m=\u001b[39medge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_attr)\n\u001b[1;32m    121\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(h1)\n\u001b[0;32m--> 123\u001b[0m h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m h \u001b[38;5;241m=\u001b[39m h2\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    125\u001b[0m h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(h2)\n",
      "File \u001b[0;32m~/.conda/envs/tam_geometric/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tam_geometric/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tam_geometric/lib/python3.11/site-packages/torch_geometric/nn/models/graph_unet.py:124\u001b[0m, in \u001b[0;36mGraphUNet.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m    121\u001b[0m     up[perm] \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    122\u001b[0m     x \u001b[38;5;241m=\u001b[39m res \u001b[38;5;241m+\u001b[39m up \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_res \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((res, up), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_convs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/tam_geometric/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tam_geometric/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tam_geometric/lib/python3.11/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/.conda/envs/tam_geometric/lib/python3.11/site-packages/torch_geometric/nn/conv/gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.conda/envs/tam_geometric/lib/python3.11/site-packages/torch_geometric/utils/loop.py:625\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    622\u001b[0m N \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m    623\u001b[0m mask \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 625\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    627\u001b[0m     loop_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, N, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tam_geometric/lib/python3.11/site-packages/torch_geometric/edge_index.py:1037\u001b[0m, in \u001b[0;36mEdgeIndex.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         edge_index\u001b[38;5;241m.\u001b[39m_indptr \u001b[38;5;241m=\u001b[39m colptr\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m edge_index\n\u001b[0;32m-> 1037\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__torch_function__\u001b[39m(\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type,\n\u001b[1;32m   1040\u001b[0m     func: Callable,\n\u001b[1;32m   1041\u001b[0m     types: Tuple[Type, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[1;32m   1042\u001b[0m     args: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m   1043\u001b[0m     kwargs: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1044\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# `EdgeIndex` should be treated as a regular PyTorch tensor for all\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;66;03m# standard PyTorch functionalities. However,\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;66;03m# * some of its metadata can be transferred to new functions, e.g.,\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;66;03m#   `torch.cat(dim=1)` can inherit the sparse matrix size, or\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;66;03m#   `torch.narrow(dim=1)` can inherit cached pointers.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;66;03m# * not all operations lead to valid `EdgeIndex` tensors again, e.g.,\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m#   `torch.sum()` does not yield a `EdgeIndex` as its output, or\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m#   `torch.cat(dim=0) violates the [2, *] shape assumption.\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;66;03m# To account for this, we hold a number of `HANDLED_FUNCTIONS` that\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# implement specific functions for valid `EdgeIndex` routines.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m HANDLED_FUNCTIONS:\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m HANDLED_FUNCTIONS[func](\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_loss = 0\n",
    "eval_loss = 0\n",
    "for epoch in range(args.n_epoch):\n",
    "    CUDA_LAUNCH_BLOCKING=1\n",
    "    torch.cuda.empty_cache()\n",
    "    ##\n",
    "    if epoch % 100 == 0:\n",
    "        if args.n_time < 100:\n",
    "            args.n_time *= 2\n",
    "    # train_loss = 0\n",
    "    for i_data in train_subset:\n",
    "        data = dataset[i_data]\n",
    "        if args.batchsize is not None:\n",
    "            train_loader = NeighborLoader(data, num_neighbors=[1], batch_size=args.batchsize)\n",
    "            for i in range(train_loader.__len__()):\n",
    "                data = next(iter(train_loader))\n",
    "                train_loss += train(model=model, data=data, args=args) / train_loader.__len__()\n",
    "        else:\n",
    "            train_loss += train(model=model, data=data, args=args)\n",
    "\n",
    "    # eval_loss = 0\n",
    "    for i_data in test_subset:\n",
    "        data = dataset[i_data]\n",
    "        if args.batchsize is not None:\n",
    "            test_loader = NeighborLoader(data, num_neighbors=[1], batch_size=args.batchsize)\n",
    "            for i in range(test_loader.__len__()):\n",
    "                data = next(iter(test_loader))\n",
    "                eval_loss += eval(model=model, data=data, args=args) / test_loader.__len__()\n",
    "        else:\n",
    "            eval_loss += eval(model=model, data=data, args=args)\n",
    "    \n",
    "    print(f'Epoch {epoch}: train loss = {train_loss}; eval loss = {eval_loss}')\n",
    "    train_loss = 0\n",
    "    eval_loss = 0\n",
    "    # else:\n",
    "        # print(f'Epoch {epoch}.')\n",
    "        \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        torch.save(model.state_dict(), f'models/{model.name}_node2_epoch{epoch+1}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tam_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
